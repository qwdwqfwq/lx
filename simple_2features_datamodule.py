import argparse
import os
import numpy as np
import pandas as pd
import torch
import pytorch_lightning as pl
from sklearn.preprocessing import StandardScaler

def load_and_preprocess_simple_features(csv_paths, window_size=32, overlap_ratio=0.5, is_test_set=False,
                                        scaler_v=None, scaler_c=None):
    """
    åŠ è½½æ•°æ®å¹¶æå–2ä¸ªåŸºç¡€ç‰¹å¾ï¼šç”µå‹ã€ç”µæµ
    
    è¿”å›:
        features: [num_samples, window_size, 2] (Voltage, Current)
        targets: [num_samples, 2] (SOC, SOE)
        original_end_indices: [num_samples] (åŸå§‹åºåˆ—ä¸­çª—å£ç»“æŸçš„æ—¶é—´æ­¥ç´¢å¼•)
    """
    all_features = []
    all_targets = []
    all_original_end_indices = []

    for csv_path in csv_paths:
        # print(f"ğŸ“‚ åŠ è½½æ•°æ®: {os.path.basename(csv_path)}") # å¯ä»¥åœ¨è¿™é‡ŒåŠ ä¸€ä¸ªæ›´è¯¦ç»†çš„æ—¥å¿—
        df = pd.read_csv(csv_path)
        # print(f"   ğŸ“Š åŸå§‹CSVæ–‡ä»¶ {os.path.basename(csv_path)} çš„è¡Œæ•°: {len(df)}")

        # æå–2ä¸ªåŸºç¡€ç‰¹å¾
        voltage = df['Voltage(V)'].values if 'Voltage(V)' in df.columns else df['Voltage'].values
        current = df['Current(A)'].values if 'Current(A)' in df.columns else df['Current'].values

        # === å…¨å±€æ ‡å‡†åŒ–ä¿®æ”¹ç‚¹1ï¼šä¸å†åœ¨æ­¤å¤„ fit_transform ===
        # ç¡®ä¿ä¼ å…¥äº† scaler
        if scaler_v is None or scaler_c is None:
            raise ValueError("å¿…é¡»æä¾›é¢„è®­ç»ƒçš„ StandardScaler (scaler_v, scaler_c) è¿›è¡Œæ•°æ®è½¬æ¢ã€‚")

        # åªè¿›è¡Œ transform
        voltage_norm = scaler_v.transform(voltage.reshape(-1, 1)).flatten()
        current_norm = scaler_c.transform(current.reshape(-1, 1)).flatten()
        # === å…¨å±€æ ‡å‡†åŒ–ä¿®æ”¹ç‚¹1 ç»“æŸ ===
        
        # æ»‘åŠ¨çª—å£
        # æ ¹æ®æ˜¯å¦ä¸ºæµ‹è¯•é›†è®¾ç½®æ­¥é•¿
        if is_test_set:
            step_size = 1 # æµ‹è¯•é›†ä½¿ç”¨æ­¥é•¿ä¸º1è¿›è¡Œæ»šåŠ¨é¢„æµ‹
            # print(f"   âš™ï¸ æµ‹è¯•é›†æ¨¡å¼: æ»‘åŠ¨çª—å£æ­¥é•¿è®¾ç½®ä¸º 1")
        else:
            step_size = int(window_size * (1 - overlap_ratio))
            if step_size < 1: step_size = 1 # ç¡®ä¿æ­¥é•¿è‡³å°‘ä¸º1
            # print(f"   âš™ï¸ è®­ç»ƒ/éªŒè¯é›†æ¨¡å¼: æ»‘åŠ¨çª—å£æ­¥é•¿è®¾ç½®ä¸º {step_size}")
        
        # ç¡®ä¿å¾ªç¯è‡³å°‘è¿è¡Œä¸€æ¬¡ï¼Œå¦‚æœæ•°æ®è¶³å¤Ÿåˆ›å»ºä¸€ä¸ªçª—å£çš„è¯
        max_i = len(df) - window_size
        if max_i < 0: # å¦‚æœæ•°æ®é•¿åº¦ä¸è¶³ä¸€ä¸ªçª—å£
            # print(f"   âš ï¸ æ•°æ®æ–‡ä»¶ {os.path.basename(csv_path)} é•¿åº¦ä¸è¶³ä¸€ä¸ªçª—å£ ({len(df)} < {window_size})ï¼Œè·³è¿‡.")
            continue # è·³è¿‡å½“å‰æ–‡ä»¶
        elif max_i == 0 and window_size == len(df): # åˆšå¥½ä¸€ä¸ªçª—å£
            # åªæœ‰ä¸€ä¸ªæ ·æœ¬ï¼Œä¸éœ€è¦å¾ªç¯ï¼Œç›´æ¥å¤„ç†
            pass
        # else: # ç§»é™¤äº†è¿™ä¸ªå¤šä½™çš„åˆ¤æ–­
        #     print(f"   âš ï¸ æ•°æ®æ–‡ä»¶ {os.path.basename(csv_path)} é•¿åº¦ä¸è¶³ä¸€ä¸ªçª—å£ ({len(df)} < {window_size})ï¼Œè·³è¿‡.")
        #     continue
        
        for i in range(0, max_i + 1, step_size): # ç¡®ä¿åŒ…å«æœ€åä¸€ä¸ªå¯èƒ½çš„çª—å£
            # ç‰¹å¾çª—å£
            v_window = voltage_norm[i:i+window_size]
            c_window = current_norm[i:i+window_size]
            
            feature_window = np.stack([v_window, c_window], axis=-1)  # [window_size, 2]
            all_features.append(feature_window)
            
            # ç›®æ ‡ï¼ˆçª—å£æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼‰
            target = np.array([df['SOC'].values[i+window_size-1], df['SOE'].values[i+window_size-1]]) # ç›´æ¥ä»dfå–soc/soe
            all_targets.append(target)
            all_original_end_indices.append(i + window_size - 1) # è®°å½•åŸå§‹åºåˆ—ä¸­çª—å£ç»“æŸçš„æ—¶é—´æ­¥ç´¢å¼•
    
    features = np.array(all_features)
    targets = np.array(all_targets)
    original_end_indices = np.array(all_original_end_indices)
    
    # print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ: ç‰¹å¾ç»´åº¦={features.shape}, ç›®æ ‡ç»´åº¦={targets.shape}, åŸå§‹ç»“æŸç´¢å¼•ç»´åº¦={original_end_indices.shape}")
    # print(f"   ç‰¹å¾æ•°é‡: 2 (Voltage, Current)")
    
    return features, targets, original_end_indices


class SimpleFeatureDataModule(pl.LightningDataModule):
    """æ•°æ®æ¨¡å— - 2ç‰¹å¾ç‰ˆ"""
    
    def __init__(self, hparams):
        super().__init__()
        self.save_hyperparameters(hparams)
        self.scaler_v = None # åˆå§‹åŒ– scaler_v
        self.scaler_c = None # åˆå§‹åŒ– scaler_c
        
    def setup(self, stage=None):
        print(f"ğŸ§ª å¼€å§‹æ•°æ®åŠ è½½ ({self.hparams.temperature}Â°C, 2ç‰¹å¾ç‰ˆ)")
        print(f"   ç‰¹å¾: Voltage + Current")

        # === å…¨å±€æ ‡å‡†åŒ–ä¿®æ”¹ç‚¹2ï¼šç¬¬ä¸€é˜¶æ®µ - æ”¶é›†è®­ç»ƒæ•°æ®å¹¶æ‹Ÿåˆæ ‡å‡†åŒ–å™¨ ===
        print("ğŸ“Š é˜¶æ®µ1: æ”¶é›†æ‰€æœ‰è®­ç»ƒæ•°æ®å¹¶æ‹Ÿåˆæ ‡å‡†åŒ–å™¨...")
        all_train_raw_voltage = []
        all_train_raw_current = []
        
        for csv_path in self.hparams.train_paths:
            print(f"   è¯»å–åŸå§‹è®­ç»ƒæ•°æ®: {os.path.basename(csv_path)}")
            df = pd.read_csv(csv_path)
            voltage = df['Voltage(V)'].values if 'Voltage(V)' in df.columns else df['Voltage'].values
            current = df['Current(A)'].values if 'Current(A)' in df.columns else df['Current'].values
            all_train_raw_voltage.extend(voltage)
            all_train_raw_current.extend(current)
            
        # æ‹Ÿåˆ StandardScaler
        self.scaler_v = StandardScaler()
        self.scaler_c = StandardScaler()
        
        self.scaler_v.fit(np.array(all_train_raw_voltage).reshape(-1, 1))
        self.scaler_c.fit(np.array(all_train_raw_current).reshape(-1, 1))
        
        print(f"âœ… StandardScaler å·²åœ¨æ‰€æœ‰è®­ç»ƒé›† Voltage (mean={self.scaler_v.mean_[0]:.4f}, std={self.scaler_v.scale_[0]:.4f}) å’Œ Current (mean={self.scaler_c.mean_[0]:.4f}, std={self.scaler_c.scale_[0]:.4f}) ä¸Šæ‹Ÿåˆå®Œæˆã€‚")
        # === å…¨å±€æ ‡å‡†åŒ–ä¿®æ”¹ç‚¹2 ç»“æŸ ===
        
        # === å…¨å±€æ ‡å‡†åŒ–ä¿®æ”¹ç‚¹3ï¼šç¬¬äºŒé˜¶æ®µ - ä½¿ç”¨ fit å¥½çš„ scaler åŠ è½½å¹¶è½¬æ¢è®­ç»ƒæ•°æ® ===
        print("ğŸ“Š é˜¶æ®µ2: ä½¿ç”¨æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨è½¬æ¢è®­ç»ƒé›†å’ŒéªŒè¯é›†æ•°æ®...")
        train_features, train_targets, train_original_end_indices = load_and_preprocess_simple_features(
            self.hparams.train_paths,
            window_size=self.hparams.window_size,
            overlap_ratio=self.hparams.overlap_ratio,
            is_test_set=False, # è®­ç»ƒé›†æ­¥é•¿ä»æŒ‰ overlap_ratio è®¡ç®—
            scaler_v=self.scaler_v, # ä¼ å…¥ fit å¥½çš„ scaler
            scaler_c=self.scaler_c  # ä¼ å…¥ fit å¥½çš„ scaler
        )
        
        # æ•°æ®è½¬æ¢
        X_train_tensor = torch.from_numpy(train_features).float()
        y_train_tensor = torch.from_numpy(train_targets).float()
        
        # æ•°æ®åˆ’åˆ†
        dataset_size = len(X_train_tensor)
        train_size = int(0.93 * dataset_size)  # 93% è®­ç»ƒ
        val_size = dataset_size - train_size   # 7% éªŒè¯
        print(f"ğŸ“Š æ•°æ®åˆ’åˆ† (93%/7%): è®­ç»ƒ={train_size:,}, éªŒè¯={val_size:,}")
        
        full_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor, torch.from_numpy(train_original_end_indices).long())
        self.train_dataset, self.val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size], 
            generator=torch.Generator().manual_seed(42)
        )
        # === å…¨å±€æ ‡å‡†åŒ–ä¿®æ”¹ç‚¹3 ç»“æŸ ===
        
        # === å…¨å±€æ ‡å‡†åŒ–ä¿®æ”¹ç‚¹4ï¼šç¬¬äºŒé˜¶æ®µ - ä½¿ç”¨ fit å¥½çš„ scaler åŠ è½½å¹¶è½¬æ¢æµ‹è¯•æ•°æ® ===
        print("ğŸ“Š é˜¶æ®µ3: ä½¿ç”¨æ‹Ÿåˆçš„æ ‡å‡†åŒ–å™¨è½¬æ¢æµ‹è¯•é›†æ•°æ®...")
        self.test_datasets = []
        for test_path in self.hparams.test_paths:
            test_features, test_targets, test_original_end_indices = load_and_preprocess_simple_features(
                [test_path],
                window_size=self.hparams.window_size,
                overlap_ratio=self.hparams.overlap_ratio, 
                is_test_set=True, # æµ‹è¯•é›†æ­¥é•¿ä¸º1
                scaler_v=self.scaler_v, # ä¼ å…¥ fit å¥½çš„ scaler
                scaler_c=self.scaler_c  # ä¼ å…¥ fit å¥½çš„ scaler
            )
            test_dataset = torch.utils.data.TensorDataset(
                torch.from_numpy(test_features).float(),
                torch.from_numpy(test_targets).float(),
                torch.from_numpy(test_original_end_indices).long() # æ·»åŠ åŸå§‹ç»“æŸç´¢å¼•
            )
            self.test_datasets.append(test_dataset)
            
        print(f"âœ… æ•°æ®å‡†å¤‡å®Œæˆ (2ç‰¹å¾ï¼Œå·²åº”ç”¨å…¨å±€æ ‡å‡†åŒ–)")
        # === å…¨å±€æ ‡å‡†åŒ–ä¿®æ”¹ç‚¹4 ç»“æŸ ===

    def train_dataloader(self): 
        return torch.utils.data.DataLoader(
            self.train_dataset, 
            batch_size=self.hparams.batch_size, 
            shuffle=True, 
            num_workers=self.hparams.num_workers, 
            pin_memory=True,
            drop_last=True
        )
    
    def val_dataloader(self): 
        return torch.utils.data.DataLoader(
            self.val_dataset, 
            batch_size=self.hparams.batch_size, 
            num_workers=self.hparams.num_workers, 
            pin_memory=True,
            drop_last=True
        )
    
    def test_dataloader(self): 
        return [torch.utils.data.DataLoader(
            ds, 
            batch_size=self.hparams.batch_size, 
            num_workers=self.hparams.num_workers, 
            pin_memory=True
        ) for ds in self.test_datasets]
