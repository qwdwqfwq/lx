"""
æ¶ˆèå®éªŒ A2.2: ç®€åŒ–ç‰¹å¾ï¼ˆä»…ç”µå‹å’Œç”µæµï¼‰ - 25Â°C
====================================================

ğŸ¯ å®éªŒç›®çš„:
    éªŒè¯ç”µåŒ–å­¦ç‰¹å¾å·¥ç¨‹çš„å¿…è¦æ€§
    é‡åŒ–é«˜çº§ç‰¹å¾ vs ç®€å•ç‰¹å¾çš„æ€§èƒ½å·®å¼‚
    
ğŸ“‰ ç§»é™¤å†…å®¹:
    âŒ æ‰€æœ‰ç”µåŒ–å­¦é«˜çº§ç‰¹å¾ï¼ˆ46ä¸ªç‰¹å¾ï¼‰
    âŒ åˆ†ç»„ç‰¹å¾ç¼–ç å™¨
    
ğŸ”„ æ›¿æ¢æ–¹æ¡ˆ:
    ä»…ä½¿ç”¨æœ€åŸºç¡€çš„2ä¸ªç‰¹å¾ï¼š
    - ç”µå‹ (Voltage)
    - ç”µæµ (Current)
    
    ç®€å•çš„ç»Ÿä¸€ç¼–ç å™¨å¤„ç†
    
âœ… ä¿ç•™å†…å®¹:
    âœ“ æ‰€æœ‰å…¶ä»–é«˜çº§ç»„ä»¶
    âœ“ å·¥å†µè‡ªé€‚åº”æœºåˆ¶
    âœ“ KAN-Transformeræ¶æ„
    
ğŸ“Š é¢„æœŸæ€§èƒ½ä¸‹é™: 20-30%
    - å¤±å»ç”µåŒ–å­¦é¢†åŸŸçŸ¥è¯†
    - å¤±å»åŠ¨æ€å“åº”ã€èƒ½é‡ã€é˜»æŠ—ã€æ¸©åº¦ç‰¹å¾
    - ä¿¡æ¯é‡å¤§å¹…å‡å°‘
    
ğŸ”¬ ç§‘å­¦æ„ä¹‰:
    â­â­â­â­â­ EnergyæœŸåˆŠæ ¸å¿ƒäº®ç‚¹ï¼
    - è¯æ˜ç”µåŒ–å­¦ç‰¹å¾å·¥ç¨‹çš„å¿…è¦æ€§
    - é‡åŒ–é¢†åŸŸçŸ¥è¯† vs åŸå§‹æ•°æ®çš„ä»·å€¼
    - ä¸ºç‰¹å¾å·¥ç¨‹æä¾›ç§‘å­¦ä¾æ®
"""

import argparse
import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
from scipy.signal import savgol_filter

from model_code_lightning import setup_chinese_font

setup_chinese_font()


def setup_sci_style():
    plt.style.use('default')
    plt.rcParams['font.family'] = ['Times New Roman', 'Arial', 'SimHei', 'DejaVu Sans']
    plt.rcParams['font.size'] = 11
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['axes.linewidth'] = 1.0
    plt.rcParams['grid.linewidth'] = 0.5
    plt.rcParams['lines.linewidth'] = 1.8


def smooth_data(data, window_length=9, polyorder=2):
    if len(data) < window_length:
        return data
    if window_length % 2 == 0:
        window_length += 1
    polyorder = min(polyorder, window_length - 1)
    try:
        return savgol_filter(data, window_length, polyorder)
    except:
        return data


def create_simple_dataset(train_paths, test_paths, window_size=32, overlap_ratio=0.5):
    """
    åˆ›å»ºç®€åŒ–æ•°æ®é›†ï¼šä»…ä½¿ç”¨ç”µå‹å’Œç”µæµ
    """
    import pandas as pd
    
    print("ğŸ”„ åˆ›å»ºç®€åŒ–ç‰¹å¾æ•°æ®é›†...")
    print("   ç‰¹å¾: ä»…ç”µå‹ + ç”µæµï¼ˆ2ä¸ªç‰¹å¾ï¼‰")
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    train_dfs = []
    for path in train_paths:
        df = pd.read_csv(path)
        # ä»…ä¿ç•™ç”µå‹å’Œç”µæµ
        df_simple = df[['Voltage', 'Current', 'SOC', 'SOE']].copy()
        train_dfs.append(df_simple)
    
    train_data = pd.concat(train_dfs, ignore_index=True)
    
    # åˆ›å»ºæ»‘åŠ¨çª—å£
    def create_windows(data, window_size, overlap_ratio):
        features = data[['Voltage', 'Current']].values
        targets = data[['SOC', 'SOE']].values
        
        step = int(window_size * (1 - overlap_ratio))
        
        X_list = []
        y_list = []
        
        for i in range(0, len(features) - window_size + 1, step):
            X_list.append(features[i:i+window_size])
            y_list.append(targets[i+window_size-1])
        
        return np.array(X_list), np.array(y_list)
    
    train_features, train_targets = create_windows(train_data, window_size, overlap_ratio)
    
    print(f"   è®­ç»ƒé›†: {train_features.shape}")
    print(f"   ç‰¹å¾ç»´åº¦: {train_features.shape[-1]} (ä»…ç”µå‹+ç”µæµ)")
    
    # å¤„ç†æµ‹è¯•é›†
    test_datasets = []
    for path in test_paths:
        df = pd.read_csv(path)
        df_simple = df[['Voltage', 'Current', 'SOC', 'SOE']].copy()
        
        test_features, test_targets = create_windows(df_simple, window_size, 0.5)
        
        test_datasets.append({
            'features': test_features,
            'targets': test_targets
        })
    
    return train_features, train_targets, test_datasets


class SimpleFeaturesModel(nn.Module):
    """
    ç®€åŒ–ç‰¹å¾æ¨¡å‹ï¼šä»…ä½¿ç”¨ç”µå‹å’Œç”µæµ
    """
    
    def __init__(self, num_heads, num_layers, num_outputs, 
                 hidden_space, dropout_rate, embed_dim, grid_size=16,
                 temperature=None):
        super().__init__()
        
        self.num_outputs = num_outputs
        self.temperature = temperature
        
        # âŒ ç§»é™¤ï¼šåˆ†ç»„ç‰¹å¾ç¼–ç å™¨
        # âœ… æ›¿æ¢ï¼šç®€å•ç»Ÿä¸€ç¼–ç å™¨ï¼ˆä»…å¤„ç†2ä¸ªç‰¹å¾ï¼šç”µå‹ã€ç”µæµï¼‰
        self.simple_encoder = nn.Sequential(
            nn.Linear(2, hidden_space//2),  # è¾“å…¥ç»´åº¦ä¸º2
            nn.LayerNorm(hidden_space//2),
            nn.GELU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_space//2, hidden_space),
            nn.LayerNorm(hidden_space),
            nn.GELU(),
            nn.Dropout(dropout_rate)
        )
        
        # âœ… ä¿ç•™ï¼šå·¥å†µæ£€æµ‹å™¨ï¼ˆåŸºäºç®€å•ç‰¹å¾ï¼‰
        self.workload_detector = nn.Sequential(
            nn.Linear(hidden_space, hidden_space//2),
            nn.LayerNorm(hidden_space//2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.1),
            nn.Linear(hidden_space//2, hidden_space//4),
            nn.ReLU(),
            nn.Linear(hidden_space//4, 2),
            nn.Softmax(dim=-1)
        )
        
        # âœ… ä¿ç•™ï¼šå·¥å†µç‰¹å®šåˆ†æ”¯
        self.udds_branch = nn.Sequential(
            nn.Linear(hidden_space, hidden_space),
            nn.LayerNorm(hidden_space),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.15)
        )
        
        self.nn_branch = nn.Sequential(
            nn.Linear(hidden_space, hidden_space + 32),
            nn.LayerNorm(hidden_space + 32),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.1),
            nn.Linear(hidden_space + 32, hidden_space + 16),
            nn.LayerNorm(hidden_space + 16),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.1),
            nn.Linear(hidden_space + 16, hidden_space),
            nn.LayerNorm(hidden_space)
        )
        
        # âœ… ä¿ç•™ï¼šå·¥å†µç‰¹å®šæ³¨æ„åŠ›
        self.nn_deep_attention = nn.ModuleList([
            nn.MultiheadAttention(
                embed_dim=hidden_space, 
                num_heads=num_heads//4,
                dropout=dropout_rate * 0.1,
                batch_first=True
            ) for _ in range(3)
        ])
        
        self.udds_attention = nn.MultiheadAttention(
            embed_dim=hidden_space, 
            num_heads=num_heads//2,
            dropout=dropout_rate * 0.2,
            batch_first=True
        )
        
        # âœ… ä¿ç•™ï¼šKAN-Transformer
        from model2 import TimeSeriesTransformer_ekan
        
        self.main_transformer = TimeSeriesTransformer_ekan(
            input_dim=hidden_space,
            num_heads=num_heads,
            num_layers=num_layers,
            num_outputs=num_outputs,
            hidden_space=hidden_space,
            dropout_rate=dropout_rate * 0.5,
            embed_dim=embed_dim,
            grid_size=grid_size,
            degree=5,
            use_residual_scaling=True
        )
        
        # âœ… ä¿ç•™ï¼šå·¥å†µç‰¹å®šé¢„æµ‹å¤´
        self.nn_prediction_head = nn.Sequential(
            nn.Linear(hidden_space, hidden_space//2),
            nn.LayerNorm(hidden_space//2),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.1),
            nn.Linear(hidden_space//2, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Linear(hidden_space//4, num_outputs)
        )
        
        self.udds_prediction_head = nn.Sequential(
            nn.Linear(hidden_space, hidden_space//3),
            nn.LayerNorm(hidden_space//3),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//3, num_outputs)
        )
        
        # âœ… ä¿ç•™ï¼šç‰©ç†çº¦æŸ
        from ablation_A8_3_baseline_25degC import BaselineConstraintLayer
        self.electrochemical_constraint = BaselineConstraintLayer(num_outputs, temperature)
        
    def forward(self, x):
        batch_size, seq_len, features = x.shape
        
        # âŒ ç®€å•ç‰¹å¾ç¼–ç ï¼ˆä»…2ä¸ªç‰¹å¾ï¼‰
        simple_encoded = self.simple_encoder(x)  # x: [batch, seq, 2]
        
        # âœ… å·¥å†µæ£€æµ‹
        workload_probs = self.workload_detector(simple_encoded.mean(dim=1))
        nn_prob = workload_probs[:, 0]
        udds_prob = workload_probs[:, 1]
        
        # âœ… å·¥å†µç‰¹å®šå¤„ç†
        udds_features = self.udds_branch(simple_encoded)
        nn_features = self.nn_branch(simple_encoded)
        
        # âœ… å·¥å†µç‰¹å®šæ³¨æ„åŠ›
        udds_attended, _ = self.udds_attention(udds_features, udds_features, udds_features)
        udds_enhanced = udds_features + udds_attended
        
        nn_enhanced = nn_features
        for attention_layer in self.nn_deep_attention:
            nn_attended, _ = attention_layer(nn_enhanced, nn_enhanced, nn_enhanced)
            nn_enhanced = nn_enhanced + nn_attended
        
        # âœ… è‡ªé€‚åº”èåˆ
        nn_weight = nn_prob.unsqueeze(1).unsqueeze(2)
        udds_weight = udds_prob.unsqueeze(1).unsqueeze(2)
        
        weighted_features = nn_enhanced * nn_weight + udds_enhanced * udds_weight
        
        # âœ… Transformerå¤„ç†
        transformer_output = self.main_transformer(weighted_features)  # å¯èƒ½æ˜¯2Dæˆ–3D
        
        # ğŸ”§ ä¿®å¤ï¼šå¤„ç†ä¸åŒç»´åº¦çš„transformerè¾“å‡º
        if len(transformer_output.shape) == 3:
            # 3Dè¾“å‡º [batch, seq_len, num_outputs] - å–æœ€åæ—¶é—´æ­¥
            transformer_features = transformer_output[:, -1, :]  # [batch, num_outputs]
        else:
            # 2Dè¾“å‡º [batch, num_outputs] - å·²ç»èšåˆå¥½äº†
            transformer_features = transformer_output  # [batch, num_outputs]
        
        # âœ… å·¥å†µç‰¹å®šé¢„æµ‹ï¼ˆä½¿ç”¨æœ€åæ—¶é—´æ­¥ï¼‰
        final_features = weighted_features[:, -1, :]  # [batch, hidden] âœ…
        nn_pred = self.nn_prediction_head(final_features)  # [batch, 2] âœ…
        udds_pred = self.udds_prediction_head(final_features)  # [batch, 2] âœ…
        
        final_prediction = (nn_pred * nn_prob.unsqueeze(1) + 
                          udds_pred * udds_prob.unsqueeze(1))  # [batch, 2] âœ…
        
        # âœ… èåˆï¼ˆç°åœ¨ç»´åº¦åŒ¹é…ï¼ï¼‰
        combined_output = 0.6 * transformer_features + 0.4 * final_prediction  # [batch, 2] âœ…
        
        # âœ… ç‰©ç†çº¦æŸï¼ˆéœ€è¦3Dè¾“å…¥ï¼‰
        combined_output_3d = combined_output.unsqueeze(1)  # [batch, 1, 2]
        constrained_output = self.electrochemical_constraint(combined_output_3d, x)
        
        # è¿”å›2D
        return constrained_output.squeeze(1)  # [batch, 2] âœ…


class SimpleFeaturesLightningModule(pl.LightningModule):
    """ç®€åŒ–ç‰¹å¾Lightningæ¨¡å—"""
    
    def __init__(self, hparams):
        super().__init__()
        if isinstance(hparams, dict): 
            hparams = argparse.Namespace(**hparams)
        self.save_hyperparameters(hparams)
        
        self.model = SimpleFeaturesModel(
            num_heads=hparams.num_heads, 
            num_layers=hparams.n_layers,
            num_outputs=len(hparams.output_features), 
            hidden_space=hparams.hidden_space,
            dropout_rate=hparams.dropout, 
            embed_dim=hparams.embed_dim, 
            grid_size=hparams.grid_size,
            temperature=getattr(hparams, 'temperature', None)
        )
        
        from ablation_A8_3_baseline_25degC import BaselinePhysicsLoss
        self.criterion = BaselinePhysicsLoss(
            electrochemical_weight=getattr(hparams, 'electrochemical_weight', 0.025)
        )
        
        self.automatic_optimization = True
        self.test_step_outputs = []
        self.current_epoch_num = 0
        
    def forward(self, x): 
        return self.model(x)
    
    def training_step(self, batch, batch_idx):
        x, y = batch
        
        if self.training:
            noise_factor = self.hparams.noise_factor * (1 - self.current_epoch_num / self.hparams.num_epochs)
            if noise_factor > 0: 
                x += torch.randn_like(x) * noise_factor
        
        y_hat = self.forward(x)
        total_loss, loss_components = self.criterion(y_hat, y, x)
        
        if torch.isnan(total_loss) or torch.isinf(total_loss): 
            total_loss = F.mse_loss(y_hat, y)
        
        self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log_dict({f'train_{k}': v for k, v in loss_components.items()}, on_step=False, on_epoch=True)
        
        with torch.no_grad(): 
            train_rmse = torch.sqrt(F.mse_loss(y_hat, y))
            self.log('train_rmse', train_rmse, on_step=False, on_epoch=True)
            
        return total_loss
    
    def validation_step(self, batch, batch_idx):
        x, y = batch
        y_hat = self.forward(x)
        val_loss = F.mse_loss(y_hat, y)
        val_rmse = torch.sqrt(val_loss)
        
        self.log('val_loss', val_loss, on_epoch=True, prog_bar=True)
        self.log('val_rmse', val_rmse, on_epoch=True, prog_bar=True)
    
    def test_step(self, batch, batch_idx, dataloader_idx=0):
        x, y = batch
        y_hat = self.forward(x)
        self.test_step_outputs[dataloader_idx].append({'y_true': y.cpu(), 'y_pred': y_hat.cpu()})
    
    def on_test_start(self): 
        self.test_step_outputs = [[] for _ in range(2)]
    
    def on_test_epoch_end(self):
        print("\n" + "="*80)
        print("ğŸ¯ æ¶ˆèå®éªŒ A2.2: ç®€åŒ–ç‰¹å¾ï¼ˆä»…ç”µå‹ç”µæµï¼‰ç»“æœ")
        print("="*80)
        print("âš ï¸  å…³é”®åŒºåˆ«:")
        print("   âŒ ç§»é™¤æ‰€æœ‰46ä¸ªç”µåŒ–å­¦ç‰¹å¾")
        print("   âŒ ç§»é™¤åˆ†ç»„ç‰¹å¾ç¼–ç å™¨")
        print("   âœ… ä»…ä½¿ç”¨2ä¸ªåŸºç¡€ç‰¹å¾ï¼šç”µå‹ + ç”µæµ")
        
        setup_sci_style()
        
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        fig.suptitle('Ablation A2.2: Simple Features (Voltage + Current Only)', 
                    fontsize=12, fontweight='normal')
        
        dataset_names = ["LA92", "UDDS"]
        subplot_idx = 0
        overall_results = {}
        
        for i, outputs in enumerate(self.test_step_outputs):
            if not outputs: continue
            
            y_true_list = []
            y_pred_list = []
            for x in outputs:
                if len(x['y_true'].shape) == 3:
                    y_true_list.append(x['y_true'][:, -1, :])
                else:
                    y_true_list.append(x['y_true'])
                    
                if len(x['y_pred'].shape) == 3:
                    y_pred_list.append(x['y_pred'][:, -1, :])
                else:
                    y_pred_list.append(x['y_pred'])
            
            try:
                y_true = torch.cat(y_true_list).numpy()
                y_pred = torch.cat(y_pred_list).numpy()
            except RuntimeError:
                fixed_y_true_list = []
                fixed_y_pred_list = []
                for yt, yp in zip(y_true_list, y_pred_list):
                    if len(yt.shape) == 2 and len(yp.shape) == 2:
                        if yt.shape[1] == yp.shape[1]:
                            fixed_y_true_list.append(yt)
                            fixed_y_pred_list.append(yp)
                
                y_true = torch.cat(fixed_y_true_list).numpy()
                y_pred = torch.cat(fixed_y_pred_list).numpy()
            
            dataset_name = dataset_names[i]
            time_axis = np.arange(len(y_true))
            
            for j, feature in enumerate(['SOC', 'SOE']):
                ax_pred = axes[i, j*2]
                actual_values = y_true[:, j] * 100
                pred_values = y_pred[:, j] * 100
                
                actual_smooth = smooth_data(actual_values)
                pred_smooth = smooth_data(pred_values)
                
                ax_pred.plot(time_axis, actual_smooth, color='#0072BD', linewidth=1.8, 
                           label='Actual', alpha=1.0)
                ax_pred.plot(time_axis, pred_smooth, color='#D95319', linewidth=1.8, 
                           label='Simple Features', alpha=1.0)
                
                ax_pred.set_xlabel('Time(s)')
                ax_pred.set_ylabel(f'{feature}(%)')
                ax_pred.set_title(f'({chr(97+subplot_idx)})')
                ax_pred.legend(loc='upper right', frameon=False)
                ax_pred.grid(True, alpha=0.2)
                ax_pred.set_ylim(0, 100)
                
                ax_error = axes[i, j*2 + 1]
                error_values = pred_smooth - actual_smooth
                error_smooth = smooth_data(error_values, window_length=7)
                
                ax_error.plot(time_axis, error_smooth, color='#1f77b4', linewidth=1.8)
                ax_error.set_xlabel('Time(s)')
                ax_error.set_ylabel(f'{feature} Error (%)')
                ax_error.set_title(f'({chr(97+subplot_idx+1)})')
                ax_error.grid(True, alpha=0.2)
                ax_error.axhline(y=0, color='black', linestyle='-', alpha=0.4)
                ax_error.set_ylim(-6, 6)
                
                rmse = np.sqrt(mean_squared_error(y_true[:, j], y_pred[:, j]))
                mae = mean_absolute_error(y_true[:, j], y_pred[:, j])
                r2 = r2_score(y_true[:, j], y_pred[:, j])
                
                result_key = f"{dataset_name}_{feature}"
                overall_results[result_key] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}
                
                print(f"ğŸ“Š {dataset_name} - {feature}:")
                print(f"    RMSE={rmse:.6f}, MAE={mae:.6f}, RÂ²={r2:.6f}")
                
                subplot_idx += 2
        
        plt.tight_layout()
        
        save_path = os.path.join(self.trainer.logger.log_dir or '.', 
                               'ablation_A2_2_simple_features_results.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nğŸ“Š ç»“æœå›¾å·²ä¿å­˜: {save_path}")
        
        results_df = pd.DataFrame(overall_results).T
        results_csv_path = os.path.join(self.trainer.logger.log_dir or '.', 
                                      'ablation_A2_2_simple_features_metrics.csv')
        results_df.to_csv(results_csv_path)
        
        avg_rmse = np.mean([r['RMSE'] for r in overall_results.values()])
        avg_mae = np.mean([r['MAE'] for r in overall_results.values()])
        avg_r2 = np.mean([r['R2'] for r in overall_results.values()])
        
        la92_rmse = np.mean([overall_results[k]['RMSE'] for k in overall_results.keys() if 'LA92' in k])
        udds_rmse = np.mean([overall_results[k]['RMSE'] for k in overall_results.keys() if 'UDDS' in k])
        
        print(f"\nğŸ† ç®€åŒ–ç‰¹å¾ç»¼åˆæ€§èƒ½:")
        print(f"    å¹³å‡RMSE: {avg_rmse:.6f}")
        print(f"    å¹³å‡MAE:  {avg_mae:.6f}")
        print(f"    å¹³å‡RÂ²:   {avg_r2:.6f}")
        
        print(f"\nğŸ“Š åˆ†å·¥å†µæ€§èƒ½:")
        print(f"    LA92å¹³å‡RMSE: {la92_rmse:.6f}")
        print(f"    UDDSå¹³å‡RMSE: {udds_rmse:.6f}")
        
        full_model_rmse = 0.020
        performance_gap = avg_rmse - full_model_rmse
        degradation_pct = (performance_gap / full_model_rmse) * 100
        
        print(f"\nğŸ“‰ vs å®Œæ•´ç”µåŒ–å­¦ç‰¹å¾ï¼ˆå®Œæ•´æ¨¡å‹ï¼‰:")
        print(f"    æ€§èƒ½ä¸‹é™: {performance_gap:.6f} ({degradation_pct:.1f}%)")
        print(f"    è¯æ˜: ç”µåŒ–å­¦ç‰¹å¾å·¥ç¨‹è´¡çŒ®äº†{degradation_pct:.1f}%çš„æ€§èƒ½æå‡ï¼")
        print(f"    ğŸ”¬ ç§‘å­¦å‘ç°:")
        print(f"       - 46ä¸ªç‰¹å¾ vs 2ä¸ªç‰¹å¾å·®è·å·¨å¤§")
        print(f"       - é¢†åŸŸçŸ¥è¯†çš„å·¨å¤§ä»·å€¼")
        print(f"       - ç‰¹å¾å·¥ç¨‹æ˜¯æ€§èƒ½å…³é”®")
        
        plt.show()
        print("="*80)
    
    def on_train_epoch_end(self): 
        self.current_epoch_num += 1
    
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=self.hparams.lr, 
            weight_decay=self.hparams.weight_decay
        )
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=15,
            min_lr=self.hparams.lr * 0.001
        )
        
        return {
            "optimizer": optimizer, 
            "lr_scheduler": {"scheduler": scheduler, "monitor": "val_loss"}
        }


class SimpleFeaturesDataModule(pl.LightningDataModule):
    """ç®€åŒ–ç‰¹å¾æ•°æ®æ¨¡å—"""
    
    def __init__(self, hparams):
        super().__init__()
        self.save_hyperparameters(hparams)
        
    def setup(self, stage=None):
        print(f"ğŸ§ª åˆ›å»ºç®€åŒ–ç‰¹å¾æ•°æ®é›†ï¼ˆä»…ç”µå‹+ç”µæµï¼‰")
        
        # ä½¿ç”¨ç®€åŒ–ç‰¹å¾æ•°æ®é›†
        train_features, train_targets, test_datasets = create_simple_dataset(
            self.hparams.train_paths, 
            self.hparams.test_paths,
            window_size=self.hparams.window_size,
            overlap_ratio=self.hparams.overlap_ratio
        )
        
        # ç¡®ä¿ç‰¹å¾ç»´åº¦æ­£ç¡®ï¼ˆåº”è¯¥æ˜¯2ï¼‰
        assert train_features.shape[-1] == 2, f"æœŸæœ›2ä¸ªç‰¹å¾ï¼Œå®é™…å¾—åˆ°{train_features.shape[-1]}ä¸ª"
        
        # æ•°æ®è½¬æ¢
        X_train_tensor = torch.from_numpy(train_features).float()
        y_train_tensor = torch.from_numpy(train_targets).float()
        
        # æ•°æ®åˆ’åˆ†
        dataset_size = len(X_train_tensor)
        train_size = int(0.93 * dataset_size)
        val_size = dataset_size - train_size
        print(f"ğŸ“Š æ•°æ®åˆ’åˆ† (93%/7%): è®­ç»ƒ={train_size:,}, éªŒè¯={val_size:,}")
        
        full_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)
        self.train_dataset, self.val_dataset = torch.utils.data.random_split(
            full_dataset, [train_size, val_size], 
            generator=torch.Generator().manual_seed(42)
        )
        
        # æµ‹è¯•æ•°æ®é›†å¤„ç†
        self.test_datasets = []
        for test_dict in test_datasets:
            test_features = test_dict['features']
            test_targets = test_dict['targets']
            test_dataset = torch.utils.data.TensorDataset(
                torch.from_numpy(test_features).float(),
                torch.from_numpy(test_targets).float()
            )
            self.test_datasets.append(test_dataset)
            
        print(f"âœ… ç®€åŒ–ç‰¹å¾æ•°æ®å‡†å¤‡å®Œæˆ (2ä¸ªç‰¹å¾ï¼šç”µå‹+ç”µæµ)")
    
    def train_dataloader(self): 
        return torch.utils.data.DataLoader(
            self.train_dataset, 
            batch_size=self.hparams.batch_size, 
            shuffle=True, 
            num_workers=self.hparams.num_workers, 
            pin_memory=True,
            drop_last=True
        )
    
    def val_dataloader(self): 
        return torch.utils.data.DataLoader(
            self.val_dataset, 
            batch_size=self.hparams.batch_size, 
            num_workers=self.hparams.num_workers, 
            pin_memory=True,
            drop_last=True
        )
    
    def test_dataloader(self): 
        return [torch.utils.data.DataLoader(
            ds, 
            batch_size=self.hparams.batch_size, 
            num_workers=self.hparams.num_workers, 
            pin_memory=True
        ) for ds in self.test_datasets]


class MemoryCleanupCallback(pl.Callback):
    """å†…å­˜æ¸…ç†å›è°ƒ"""
    def on_train_epoch_end(self, trainer, pl_module): 
        import gc
        gc.collect()
        torch.cuda.empty_cache()


def main():
    parser = argparse.ArgumentParser(description='æ¶ˆèå®éªŒ A2.2: ç®€åŒ–ç‰¹å¾ï¼ˆä»…ç”µå‹ç”µæµï¼‰')
    
    parser.add_argument('--train_paths', type=str, nargs='+', default=[
        r"C:\25degC training\03-18-17_02.17 25degC_Cycle_1_Pan18650PF.csv",
        r"C:\25degC training\03-19-17_03.25 25degC_Cycle_2_Pan18650PF.csv",
        r"C:\25degC training\03-19-17_09.07 25degC_Cycle_3_Pan18650PF.csv",
        r"C:\25degC training\03-19-17_14.31 25degC_Cycle_4_Pan18650PF.csv",
        r"C:\25degC training\03-20-17_01.43 25degC_US06_Pan18650PF.csv",
        r"C:\25degC training\03-20-17_05.56 25degC_HWFTa_Pan18650PF.csv",
        r"C:\25degC testing\03-21-17_16.27 25degC_NN_Pan18650PF.csv"
    ])
    parser.add_argument('--test_paths', type=str, nargs='+', default=[
        r"C:\25degC training\03-21-17_09.38 25degC_LA92_Pan18650PF.csv",
        r"C:\25degC testing\03-21-17_00.29 25degC_UDDS_Pan18650PF.csv"
    ])
    parser.add_argument('--result_dir', type=str, default='ablation_A2_2_simple_features_results')
    parser.add_argument('--output_features', type=str, default='SOC,SOE')
    parser.add_argument('--window_size', type=int, default=32)
    parser.add_argument('--overlap_ratio', type=float, default=0.5)
    parser.add_argument('--temperature', type=float, default=25.0)
    
    parser.add_argument('--n_layers', type=int, default=5)
    parser.add_argument('--num_heads', type=int, default=16)
    parser.add_argument('--hidden_space', type=int, default=128)
    parser.add_argument('--embed_dim', type=int, default=128)
    parser.add_argument('--dropout', type=float, default=0.20)
    parser.add_argument('--weight_decay', type=float, default=0.0006)
    parser.add_argument('--noise_factor', type=float, default=0.004)
    parser.add_argument('--electrochemical_weight', type=float, default=0.025)
    parser.add_argument('--grid_size', type=int, default=24)
    parser.add_argument('--num_epochs', type=int, default=200)
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--patience', type=int, default=30)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--lr', type=float, default=0.0006)
    parser.add_argument('--num_workers', type=int, default=0)
    parser.add_argument('--gradient_accumulation_steps', type=int, default=2)
    
    args = parser.parse_args()
    args.output_features = [item.strip() for item in args.output_features.split(',')]
    os.makedirs(args.result_dir, exist_ok=True)
    
    print("ğŸ”¬ === æ¶ˆèå®éªŒ A2.2: ç®€åŒ–ç‰¹å¾ï¼ˆä»…ç”µå‹ç”µæµï¼‰ ===")
    print("â­â­â­â­â­ EnergyæœŸåˆŠæ ¸å¿ƒäº®ç‚¹ï¼")
    print("\nğŸ“‰ ç§»é™¤çš„ç»„ä»¶:")
    print("   âŒ æ‰€æœ‰46ä¸ªç”µåŒ–å­¦ç‰¹å¾")
    print("   âŒ åˆ†ç»„ç‰¹å¾ç¼–ç å™¨")
    print("   âŒ åŠ¨æ€å“åº”ã€èƒ½é‡ã€é˜»æŠ—ã€æ¸©åº¦ç‰¹å¾")
    print("\nğŸ”„ æ›¿æ¢æ–¹æ¡ˆ:")
    print("   ä»…ä½¿ç”¨2ä¸ªåŸºç¡€ç‰¹å¾ï¼š")
    print("   - ç”µå‹ (Voltage)")
    print("   - ç”µæµ (Current)")
    print("\nâœ… ä¿ç•™çš„ç»„ä»¶:")
    print("   âœ“ æ‰€æœ‰å…¶ä»–é«˜çº§ç»„ä»¶")
    print("   âœ“ å·¥å†µè‡ªé€‚åº”æœºåˆ¶")
    print("   âœ“ KAN-Transformeræ¶æ„")
    print("\nğŸ“Š é¢„æœŸæ€§èƒ½ä¸‹é™: 20-30%")
    print("   ç›®çš„: éªŒè¯ç”µåŒ–å­¦ç‰¹å¾å·¥ç¨‹çš„å¿…è¦æ€§")
    print("\nğŸ”¬ ç§‘å­¦æ„ä¹‰:")
    print("   - é‡åŒ–é¢†åŸŸçŸ¥è¯†çš„ä»·å€¼")
    print("   - è¯æ˜ç‰¹å¾å·¥ç¨‹çš„å¿…è¦æ€§")
    print("   - ä¸ºç”µæ± ç®¡ç†ç³»ç»Ÿæä¾›è®¾è®¡æŒ‡å¯¼")
    
    try:
        pl.seed_everything(args.seed, workers=True)
        datamodule = SimpleFeaturesDataModule(args)
        datamodule.setup()
        
        model = SimpleFeaturesLightningModule(args)
        
        checkpoint_callback = ModelCheckpoint(
            dirpath=args.result_dir, 
            filename='ablation-simple-features-{epoch:02d}-{val_loss:.6f}',
            save_top_k=1, monitor='val_loss', mode='min'
        )
        early_stop_callback = EarlyStopping(
            monitor='val_loss', patience=args.patience, mode='min'
        )
        lr_monitor = LearningRateMonitor(logging_interval='epoch')
        
        trainer = pl.Trainer(
            max_epochs=args.num_epochs, 
            accelerator='auto', 
            devices=1,
            callbacks=[checkpoint_callback, early_stop_callback, lr_monitor, MemoryCleanupCallback()],
            precision='32',
            gradient_clip_val=0.5,
            accumulate_grad_batches=args.gradient_accumulation_steps
        )
        
        print(f"\nğŸš€ å¼€å§‹ç®€åŒ–ç‰¹å¾è®­ç»ƒ...")
        trainer.fit(model, datamodule)
        
        print(f"\nğŸ“Š æµ‹è¯•ç®€åŒ–ç‰¹å¾æ¨¡å‹...")
        trainer.test(model, datamodule=datamodule, ckpt_path='best')
        
        print(f"\nâœ… æ¶ˆèå®éªŒ A2.2 å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
