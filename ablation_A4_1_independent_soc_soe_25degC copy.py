"""
æ¶ˆèå®éªŒ A4.1: SOCå’ŒSOEç‹¬ç«‹é¢„æµ‹ vs è”åˆé¢„æµ‹ - 25Â°C
==========================================================

ğŸ¯ å®éªŒç›®çš„:
    éªŒè¯SOC-SOEè”åˆé¢„æµ‹çš„ä»·å€¼
    é‡åŒ–ä¿¡æ¯å…±äº«å’Œç‰©ç†è€¦åˆçº¦æŸçš„è´¡çŒ®
    
ğŸ“‰ ç§»é™¤å†…å®¹:
    âŒ SOC-SOEè”åˆé¢„æµ‹ï¼ˆå•ä¸€æ¨¡å‹åŒæ—¶è¾“å‡º2ä¸ªå€¼ï¼‰
    âŒ SOC-SOEè€¦åˆçº¦æŸ (|SOE - SOC| < 0.12)
    âŒ å…±äº«ç‰¹å¾ç¼–ç å™¨
    
ğŸ”„ æ›¿æ¢æ–¹æ¡ˆ:
    ä¸¤ä¸ªå®Œå…¨ç‹¬ç«‹çš„æ¨¡å‹ï¼š
    - SOCæ¨¡å‹ï¼šç‹¬ç«‹ç¼–ç å™¨ + Transformer + é¢„æµ‹å¤´ â†’ SOC
    - SOEæ¨¡å‹ï¼šç‹¬ç«‹ç¼–ç å™¨ + Transformer + é¢„æµ‹å¤´ â†’ SOE
    
âœ… ä¿ç•™å†…å®¹:
    âœ“ ç›¸åŒçš„æ¶æ„å¤æ‚åº¦ï¼ˆæ¯ä¸ªæ¨¡å‹ï¼‰
    âœ“ ç›¸åŒçš„è®­ç»ƒç­–ç•¥
    âœ“ ç›¸åŒçš„è¶…å‚æ•°
    
ğŸ“Š é¢„æœŸæ€§èƒ½ä¸‹é™: 8-12%
    - å¤±å»SOC-SOEä¿¡æ¯å…±äº«
    - å¤±å»ç‰©ç†è€¦åˆçº¦æŸ
    - è®¡ç®—é‡ç¿»å€ä½†æ— ååŒæ•ˆåº”
    
ğŸ”¬ ç§‘å­¦æ„ä¹‰:
    â­â­â­â­â­ EnergyæœŸåˆŠæ ¸å¿ƒäº®ç‚¹ï¼
    - è¯æ˜è”åˆå­¦ä¹ çš„ä¼˜åŠ¿
    - é‡åŒ–ç‰©ç†è€¦åˆçº¦æŸçš„ä»·å€¼
    - ä¸ºç”µæ± ç®¡ç†ç³»ç»Ÿæä¾›è®¾è®¡æŒ‡å¯¼

æ•°æ®æ ‡å‡†åŒ–ï¼šä½¿ç”¨å…¨å±€MinMaxScaler
=================================
ğŸ”„ æ›¿ä»£æ–¹æ¡ˆï¼š
    å…¨å±€MinMaxScaler: é’ˆå¯¹46ç»´ç”µåŒ–å­¦ç‰¹å¾è¿›è¡Œ0-1æ ‡å‡†åŒ–ï¼Œé¿å…StandardScalerå¯èƒ½å¸¦æ¥çš„è´Ÿå€¼å’Œè¿‡å¤§æ–¹å·®é—®é¢˜ã€‚
    
æ•ˆæœé¢„æœŸï¼š
    ç”±äºKANå¯¹è¾“å…¥èŒƒå›´æ•æ„Ÿï¼ŒMinMaxScaleré¢„è®¡èƒ½æä¾›æ›´ç¨³å®šçš„è®­ç»ƒå’Œæ›´å¥½çš„æ€§èƒ½ã€‚
"""

import argparse
import os
import gc
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback, LearningRateMonitor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
from scipy.signal import savgol_filter

# from electrochemical_features import create_electrochemical_dataset # ç§»é™¤æ—§çš„å¯¼å…¥
# from electrochemical_46features_datamodule import Electrochemical46FeaturesDataModule # ç§»é™¤æ—§çš„æ•°æ®æ¨¡å—å¯¼å…¥
from electrochemical_46features_kan_datamodule import Electrochemical46FeaturesKANDataModule # å¯¼å…¥æ–°çš„KANæ•°æ®æ¨¡å—

from model_code_lightning import setup_chinese_font

setup_chinese_font()


def setup_sci_style():
    """SCIè®ºæ–‡çº§åˆ«æ ·å¼"""
    plt.style.use('default')
    plt.rcParams['font.family'] = ['Times New Roman', 'Arial', 'SimHei', 'DejaVu Sans']
    plt.rcParams['font.size'] = 11
    plt.rcParams['axes.unicode_minus'] = False
    plt.rcParams['axes.linewidth'] = 1.0
    plt.rcParams['grid.linewidth'] = 0.5
    plt.rcParams['lines.linewidth'] = 1.8


def smooth_data(data, window_length=9, polyorder=2):
    if len(data) < window_length:
        return data
    if window_length % 2 == 0:
        window_length += 1
    polyorder = min(polyorder, window_length - 1)
    try:
        return savgol_filter(data, window_length, polyorder)
    except:
        return data


class IndependentSOCModel(nn.Module):
    """
    ç‹¬ç«‹çš„SOCé¢„æµ‹æ¨¡å‹
    å®Œå…¨ç‹¬ç«‹çš„æ¶æ„ï¼Œä¸ä¸SOEå…±äº«ä»»ä½•ä¿¡æ¯
    """
    
    def __init__(self, input_dim, num_heads, num_layers, hidden_space, 
                 dropout_rate, embed_dim, grid_size=16, temperature=None):
        super().__init__()
        
        self.input_dim = input_dim
        self.temperature = temperature
        
        # âœ… ç‹¬ç«‹çš„ç‰¹å¾ç¼–ç å™¨ï¼ˆä¸å®Œæ•´æ¨¡å‹ç›¸åŒçš„æ¶æ„ï¼Œä½†å‚æ•°ç‹¬ç«‹ï¼‰
        self.basic_encoder = nn.Sequential(
            nn.Linear(10, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//4, hidden_space//4),
            nn.LayerNorm(hidden_space//4)
        )
        
        self.dynamic_encoder = nn.Sequential(
            nn.Linear(12, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//4, hidden_space//4),
            nn.LayerNorm(hidden_space//4)
        )
        
        self.energy_encoder = nn.Sequential(
            nn.Linear(8, hidden_space//6),
            nn.LayerNorm(hidden_space//6),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//6, hidden_space//6),
            nn.LayerNorm(hidden_space//6)
        )
        
        self.impedance_encoder = nn.Sequential(
            nn.Linear(10, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//4, hidden_space//4),
            nn.LayerNorm(hidden_space//4)
        )
        
        self.temperature_encoder = nn.Sequential(
            nn.Linear(6, hidden_space//6),
            nn.LayerNorm(hidden_space//6),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//6, hidden_space//6),
            nn.LayerNorm(hidden_space//6)
        )
        
        # âœ… ç‹¬ç«‹çš„å¤„ç†åˆ†æ”¯
        total_encoded_dim = hidden_space//4 * 3 + hidden_space//6 * 2
        
        self.soc_branch = nn.Sequential(
            nn.Linear(total_encoded_dim, hidden_space),
            nn.LayerNorm(hidden_space),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.15)
        )
        
        # âœ… ç‹¬ç«‹çš„Transformer
        from model2 import TimeSeriesTransformer_ekan
        
        self.transformer = TimeSeriesTransformer_ekan(
            input_dim=hidden_space,
            num_heads=num_heads,
            num_layers=num_layers,
            num_outputs=1,  # ä»…è¾“å‡ºSOC
            hidden_space=hidden_space,
            dropout_rate=dropout_rate * 0.5,
            embed_dim=embed_dim,
            grid_size=grid_size,
            degree=5,
            use_residual_scaling=True
        )
        
        # âœ… ç‹¬ç«‹çš„é¢„æµ‹å¤´
        self.prediction_head = nn.Sequential(
            nn.Linear(hidden_space, hidden_space//2),
            nn.LayerNorm(hidden_space//2),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.1),
            nn.Linear(hidden_space//2, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Linear(hidden_space//4, 1)  # ä»…è¾“å‡ºSOC
        )
        
    def forward(self, x):
        # ç‰¹å¾ç¼–ç 
        basic_encoded = self.basic_encoder(x[:, :, :10])
        dynamic_encoded = self.dynamic_encoder(x[:, :, 10:22])
        energy_encoded = self.energy_encoder(x[:, :, 22:30])
        impedance_encoded = self.impedance_encoder(x[:, :, 30:40])
        temp_encoded = self.temperature_encoder(x[:, :, 40:46])
        
        concatenated = torch.cat([
            basic_encoded, dynamic_encoded, energy_encoded, 
            impedance_encoded, temp_encoded
        ], dim=-1)
        
        # SOCç‰¹å®šå¤„ç†
        soc_features = self.soc_branch(concatenated)
        
        # Transformerå¤„ç†
        transformer_output = self.transformer(soc_features)
        
        # é¢„æµ‹
        prediction = self.prediction_head(soc_features.mean(dim=1))
        
        # èåˆ
        combined = 0.6 * transformer_output + 0.4 * prediction.unsqueeze(1)
        
        # åŸºç¡€çº¦æŸï¼ˆæ— SOEè€¦åˆï¼‰
        constrained = torch.sigmoid(combined)
        
        return constrained


class IndependentSOEModel(nn.Module):
    """
    ç‹¬ç«‹çš„SOEé¢„æµ‹æ¨¡å‹
    å®Œå…¨ç‹¬ç«‹çš„æ¶æ„ï¼Œä¸ä¸SOCå…±äº«ä»»ä½•ä¿¡æ¯
    """
    
    def __init__(self, input_dim, num_heads, num_layers, hidden_space, 
                 dropout_rate, embed_dim, grid_size=16, temperature=None):
        super().__init__()
        
        self.input_dim = input_dim
        self.temperature = temperature
        
        # âœ… ç‹¬ç«‹çš„ç‰¹å¾ç¼–ç å™¨ï¼ˆæ¶æ„ç›¸åŒä½†å‚æ•°å®Œå…¨ç‹¬ç«‹ï¼‰
        self.basic_encoder = nn.Sequential(
            nn.Linear(10, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//4, hidden_space//4),
            nn.LayerNorm(hidden_space//4)
        )
        
        self.dynamic_encoder = nn.Sequential(
            nn.Linear(12, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//4, hidden_space//4),
            nn.LayerNorm(hidden_space//4)
        )
        
        self.energy_encoder = nn.Sequential(
            nn.Linear(8, hidden_space//6),
            nn.LayerNorm(hidden_space//6),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//6, hidden_space//6),
            nn.LayerNorm(hidden_space//6)
        )
        
        self.impedance_encoder = nn.Sequential(
            nn.Linear(10, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//4, hidden_space//4),
            nn.LayerNorm(hidden_space//4)
        )
        
        self.temperature_encoder = nn.Sequential(
            nn.Linear(6, hidden_space//6),
            nn.LayerNorm(hidden_space//6),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//6, hidden_space//6),
            nn.LayerNorm(hidden_space//6)
        )
        
        # âœ… ç‹¬ç«‹çš„å¤„ç†åˆ†æ”¯
        total_encoded_dim = hidden_space//4 * 3 + hidden_space//6 * 2
        
        self.soe_branch = nn.Sequential(
            nn.Linear(total_encoded_dim, hidden_space),
            nn.LayerNorm(hidden_space),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.15)
        )
        
        # âœ… ç‹¬ç«‹çš„Transformer
        from model2 import TimeSeriesTransformer_ekan
        
        self.transformer = TimeSeriesTransformer_ekan(
            input_dim=hidden_space,
            num_heads=num_heads,
            num_layers=num_layers,
            num_outputs=1,  # ä»…è¾“å‡ºSOE
            hidden_space=hidden_space,
            dropout_rate=dropout_rate * 0.5,
            embed_dim=embed_dim,
            grid_size=grid_size,
            degree=5,
            use_residual_scaling=True
        )
        
        # âœ… ç‹¬ç«‹çš„é¢„æµ‹å¤´
        self.prediction_head = nn.Sequential(
            nn.Linear(hidden_space, hidden_space//2),
            nn.LayerNorm(hidden_space//2),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.1),
            nn.Linear(hidden_space//2, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Linear(hidden_space//4, 1)  # ä»…è¾“å‡ºSOE
        )
        
    def forward(self, x):
        # ç‰¹å¾ç¼–ç 
        basic_encoded = self.basic_encoder(x[:, :, :10])
        dynamic_encoded = self.dynamic_encoder(x[:, :, 10:22])
        energy_encoded = self.energy_encoder(x[:, :, 22:30])
        impedance_encoded = self.impedance_encoder(x[:, :, 30:40])
        temp_encoded = self.temperature_encoder(x[:, :, 40:46])
        
        concatenated = torch.cat([
            basic_encoded, dynamic_encoded, energy_encoded, 
            impedance_encoded, temp_encoded
        ], dim=-1)
        
        # SOEç‰¹å®šå¤„ç†
        soe_features = self.soe_branch(concatenated)
        
        # Transformerå¤„ç†
        transformer_output = self.transformer(soe_features)
        
        # é¢„æµ‹
        prediction = self.prediction_head(soe_features.mean(dim=1))
        
        # èåˆ
        combined = 0.6 * transformer_output + 0.4 * prediction.unsqueeze(1)
        
        # åŸºç¡€çº¦æŸï¼ˆæ— SOCè€¦åˆï¼‰
        constrained = torch.sigmoid(combined)
        
        return constrained


class IndependentSOCSOELightningModule(pl.LightningModule):
    """
    ç‹¬ç«‹é¢„æµ‹Lightningæ¨¡å—
    ä½¿ç”¨ä¸¤ä¸ªå®Œå…¨ç‹¬ç«‹çš„æ¨¡å‹
    """
    
    def __init__(self, hparams):
        super().__init__()
        if isinstance(hparams, dict): 
            hparams = argparse.Namespace(**hparams)
        self.save_hyperparameters(hparams)
        
        # âœ… ä¸¤ä¸ªå®Œå…¨ç‹¬ç«‹çš„æ¨¡å‹
        self.soc_model = IndependentSOCModel(
            input_dim=46,
            num_heads=hparams.num_heads, 
            num_layers=hparams.n_layers,
            hidden_space=hparams.hidden_space,
            dropout_rate=hparams.dropout, 
            embed_dim=hparams.embed_dim, 
            grid_size=hparams.grid_size,
            temperature=getattr(hparams, 'temperature', None)
        )
        
        self.soe_model = IndependentSOEModel(
            input_dim=46,
            num_heads=hparams.num_heads, 
            num_layers=hparams.n_layers,
            hidden_space=hparams.hidden_space,
            dropout_rate=hparams.dropout, 
            embed_dim=hparams.embed_dim, 
            grid_size=hparams.grid_size,
            temperature=getattr(hparams, 'temperature', None)
        )
        
        self.automatic_optimization = True
        self.test_step_outputs = []
        self.current_epoch_num = 0
        
    def forward(self, x):
        # åˆ†åˆ«é¢„æµ‹SOCå’ŒSOEï¼ˆå®Œå…¨ç‹¬ç«‹ï¼‰
        soc_pred = self.soc_model(x)
        soe_pred = self.soe_model(x)
        
        # âŒ æ— è€¦åˆçº¦æŸï¼ç›´æ¥æ‹¼æ¥
        combined = torch.cat([soc_pred, soe_pred], dim=-1)
        
        return combined
    
    def training_step(self, batch, batch_idx):
        x, y, _ = batch # è§£åŒ…ï¼Œå¿½ç•¥åŸå§‹ç´¢å¼•
        
        # æ•°æ®å¢å¼º
        if self.training:
            noise_factor = self.hparams.noise_factor * (1 - self.current_epoch_num / self.hparams.num_epochs)
            if noise_factor > 0: 
                x += torch.randn_like(x) * noise_factor
        
        # ç‹¬ç«‹é¢„æµ‹
        y_hat = self.forward(x)  # [batch, seq_len, 2]
        
        # ğŸ”§ ä¿®å¤ç»´åº¦ï¼šy_hatæ˜¯[batch, seq_len, 2]ï¼Œéœ€è¦å–æœ€åæ—¶é—´æ­¥
        if len(y_hat.shape) == 3:
            y_hat_final = y_hat[:, -1, :]  # [batch, 2] - å–æœ€åæ—¶é—´æ­¥
        else:
            y_hat_final = y_hat  # [batch, 2]
        
        # åˆ†åˆ«è®¡ç®—SOCå’ŒSOEçš„æŸå¤±
        soc_loss = F.mse_loss(y_hat_final[:, 0], y[:, 0])
        soe_loss = F.mse_loss(y_hat_final[:, 1], y[:, 1])
        
        # âŒ æ— è”åˆæŸå¤±ï¼Œæ— è€¦åˆçº¦æŸ
        total_loss = soc_loss + soe_loss
        
        if torch.isnan(total_loss) or torch.isinf(total_loss): 
            total_loss = F.mse_loss(y_hat_final, y)
        
        self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log('train_soc_loss', soc_loss, on_step=False, on_epoch=True)
        self.log('train_soe_loss', soe_loss, on_step=False, on_epoch=True)
        
        with torch.no_grad(): 
            train_rmse = torch.sqrt(F.mse_loss(y_hat_final, y))
            self.log('train_rmse', train_rmse, on_step=False, on_epoch=True)
            
        return total_loss
    
    def validation_step(self, batch, batch_idx):
        x, y, _ = batch # è§£åŒ…ï¼Œå¿½ç•¥åŸå§‹ç´¢å¼•
        y_hat = self.forward(x)  # [batch, seq_len, 2]
        
        # å–æœ€åæ—¶é—´æ­¥
        if len(y_hat.shape) == 3:
            y_hat_final = y_hat[:, -1, :]  # [batch, 2]
        else:
            y_hat_final = y_hat  # [batch, 2]
        
        val_loss = F.mse_loss(y_hat_final, y)
        val_rmse = torch.sqrt(val_loss)
        
        self.log('val_loss', val_loss, on_epoch=True, prog_bar=True)
        self.log('val_rmse', val_rmse, on_epoch=True, prog_bar=True)
    
    def test_step(self, batch, batch_idx, dataloader_idx=0):
        x, y, original_end_indices = batch # è§£åŒ…åŸå§‹ç´¢å¼•
        y_hat = self.forward(x)  # [batch, seq_len, 2]
        
        # å–æœ€åæ—¶é—´æ­¥
        if len(y_hat.shape) == 3:
            y_hat_final = y_hat[:, -1, :]  # [batch, 2]
        else:
            y_hat_final = y_hat  # [batch, 2]
        
        self.test_step_outputs[dataloader_idx].append({'y_true': y.cpu(), 'y_pred': y_hat_final.cpu(), 'original_end_indices': original_end_indices.cpu()})
    
    def on_test_start(self): 
        self.test_step_outputs = [[] for _ in range(2)]
    
    def on_test_epoch_end(self):
        print("\n" + "="*80)
        print("ğŸ¯ æ¶ˆèå®éªŒ A4.1: SOC-SOEç‹¬ç«‹é¢„æµ‹ç»“æœ")
        print("="*80)
        print("âš ï¸  å…³é”®åŒºåˆ«:")
        print("   âŒ æ— SOC-SOEä¿¡æ¯å…±äº«")
        print("   âŒ æ— ç‰©ç†è€¦åˆçº¦æŸ")
        print("   âŒ è®¡ç®—é‡ç¿»å€")
        print("ğŸ“Š æ•°æ®æ ‡å‡†åŒ–ï¼šå…¨å±€MinMaxScaler")
        
        setup_sci_style()
        
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        fig.suptitle('Ablation A4.1: Independent SOC-SOE Prediction (No Information Sharing, MinMaxScaler)', 
                    fontsize=12, fontweight='normal')
        
        dataset_names = ["LA92", "UDDS"]
        subplot_idx = 0
        overall_results = {}
        
        for i, outputs in enumerate(self.test_step_outputs):
            if not outputs: continue
            
            y_true_list = []
            y_pred_list = []
            original_end_indices_list = [] # æ–°å¢ï¼šç”¨äºæ”¶é›†åŸå§‹ç´¢å¼•
            for x in outputs:
                if len(x['y_true'].shape) == 3:
                    y_true_list.append(x['y_true'][:, -1, :])
                    original_end_indices_list.append(x['original_end_indices'][:, -1]) # æ”¶é›†åŸå§‹ç´¢å¼•
                else:
                    y_true_list.append(x['y_true'])
                    original_end_indices_list.append(x['original_end_indices']) # æ”¶é›†åŸå§‹ç´¢å¼•
                    
                if len(x['y_pred'].shape) == 3:
                    y_pred_list.append(x['y_pred'][:, -1, :])
                else:
                    y_pred_list.append(x['y_pred'])
            
            try:
                y_true = torch.cat(y_true_list).numpy()
                y_pred = torch.cat(y_pred_list).numpy()
                original_end_indices = torch.cat(original_end_indices_list).numpy() # åˆå¹¶åŸå§‹ç´¢å¼•
            except RuntimeError:
                fixed_y_true_list = []
                fixed_y_pred_list = []
                fixed_original_end_indices_list = [] # æ–°å¢ï¼šç”¨äºæ”¶é›†ä¿®å¤åçš„åŸå§‹ç´¢å¼•
                for yt, yp, oei in zip(y_true_list, y_pred_list, original_end_indices_list):
                    if len(yt.shape) == 2 and len(yp.shape) == 2:
                        if yt.shape[1] == yp.shape[1]:
                            fixed_y_true_list.append(yt)
                            fixed_y_pred_list.append(yp)
                            fixed_original_end_indices_list.append(oei) # æ”¶é›†ä¿®å¤åçš„åŸå§‹ç´¢å¼•
                
                y_true = torch.cat(fixed_y_true_list).numpy()
                y_pred = torch.cat(fixed_y_pred_list).numpy()
                original_end_indices = torch.cat(fixed_original_end_indices_list).numpy() # åˆå¹¶ä¿®å¤åçš„åŸå§‹ç´¢å¼•
            
            dataset_name = dataset_names[i]
            # åŒæ—¶ä¿å­˜åŸå§‹æ—¶é—´è½´ç´¢å¼•ï¼Œä»¥ä¾¿ç”Ÿæˆ_true.npyå’Œ_pred.npyæ—¶ä½œä¸ºæ¨ªåæ ‡
            np.save(os.path.join(self.trainer.logger.log_dir or '.', f'ablation_A4_1_independent_MinMaxScaler_time_axis_{dataset_name}.npy'), original_end_indices)
            print(f"   ğŸ“Š ablation_A4_1_independent_MinMaxScaler_time_axis_{dataset_name}.npy å·²ä¿å­˜")
            # --- æ–°å¢ç»“æŸ ---

            time_axis = original_end_indices # ä½¿ç”¨åŸå§‹æ—¶é—´è½´ç´¢å¼•ä½œä¸ºæ¨ªåæ ‡
            
            for j, feature in enumerate(['SOC', 'SOE']):
                ax_pred = axes[i, j*2]
                actual_values = y_true[:, j] * 100
                pred_values = y_pred[:, j] * 100
                
                actual_smooth = smooth_data(actual_values, window_length=9, polyorder=2)
                pred_smooth = smooth_data(pred_values, window_length=9, polyorder=2)
                
                ax_pred.plot(time_axis, actual_smooth, color='#0072BD', linewidth=1.8, 
                           label='Actual Value', alpha=1.0)
                ax_pred.plot(time_axis, pred_smooth, color='#D95319', linewidth=1.8, 
                           label='Independent', alpha=1.0)
                
                ax_pred.set_xlabel('Time(s)', fontsize=11)
                ax_pred.set_ylabel(f'{feature}(%)', fontsize=11)
                ax_pred.set_title(f'({chr(97+subplot_idx)})', fontsize=12, fontweight='normal')
                ax_pred.legend(loc='upper right', frameon=False, fontsize=10)
                ax_pred.grid(True, alpha=0.2, linestyle='-', linewidth=0.5)
                ax_pred.set_ylim(0, 100)

                for spine in ax_pred.spines.values():
                    spine.set_linewidth(1.0)
                    spine.set_color('black')
                
                ax_error = axes[i, j*2 + 1]
                error_values = pred_smooth - actual_smooth
                error_smooth = smooth_data(error_values, window_length=7, polyorder=2)
                
                ax_error.plot(time_axis, error_smooth, color='#1f77b4', linewidth=1.8, alpha=1.0)
                ax_error.set_xlabel('Time(s)', fontsize=11)
                ax_error.set_ylabel(f'{feature} Error (%)', fontsize=11)
                ax_error.set_title(f'({chr(97+subplot_idx+1)})', fontsize=12, fontweight='normal')
                ax_error.grid(True, alpha=0.2, linestyle='-', linewidth=0.5)
                ax_error.axhline(y=0, color='black', linestyle='-', alpha=0.4, linewidth=0.8)
                ax_error.set_ylim(-6, 6)

                for spine in ax_error.spines.values():
                    spine.set_linewidth(1.0)
                    spine.set_color('black')
                
                rmse = np.sqrt(mean_squared_error(y_true[:, j], y_pred[:, j]))
                mae = mean_absolute_error(y_true[:, j], y_pred[:, j])
                r2 = r2_score(y_true[:, j], y_pred[:, j])
                
                result_key = f"{dataset_name}_{feature}"
                overall_results[result_key] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}
                
                print(f"ğŸ“Š {dataset_name} - {feature} (ç‹¬ç«‹é¢„æµ‹): ")
                print(f"    RMSE={rmse:.6f}, MAE={mae:.6f}, RÂ²={r2:.6f}")
                
                subplot_idx += 2
        
        plt.tight_layout()
        plt.subplots_adjust(top=0.95, bottom=0.15, left=0.06, right=0.98, 
                           hspace=0.35, wspace=0.25)
        
        save_path = os.path.join(self.trainer.logger.log_dir or '.', 
                               'ablation_A4_1_independent_MinMaxScaler_results.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white', 
                    edgecolor='none', format='png', pad_inches=0.1)
        print(f"\nğŸ“Š ç»“æœå›¾å·²ä¿å­˜: {save_path}")
        
        results_df = pd.DataFrame(overall_results).T
        results_df.index.name = 'Dataset_Feature'
        results_csv_path = os.path.join(self.trainer.logger.log_dir or '.', 
                                      'ablation_A4_1_independent_MinMaxScaler_metrics.csv')
        results_df.to_csv(results_csv_path)
        print(f"ğŸ“Š æµ‹è¯•æŒ‡æ ‡ç»“æœå·²ä¿å­˜: {results_csv_path}")
        
        avg_rmse = np.mean([r['RMSE'] for r in overall_results.values()])
        avg_mae = np.mean([r['MAE'] for r in overall_results.values()])
        avg_r2 = np.mean([r['R2'] for r in overall_results.values()])
        
        la92_rmse = np.mean([overall_results[k]['RMSE'] for k in overall_results.keys() if 'LA92' in k])
        udds_rmse = np.mean([overall_results[k]['RMSE'] for k in overall_results.keys() if 'UDDS' in k])
        
        # è®¡ç®—SOC-SOEç›¸å…³æ€§å·®å¼‚
        # ä¿®æ­£ï¼šy_true å’Œ y_pred å·²ç»æ˜¯ [batch, 2] å½¢çŠ¶ï¼Œå¯ä»¥ç›´æ¥ç´¢å¼•
        soc_values = y_true[:, 0]
        soe_values = y_true[:, 1]
        soc_pred_values = y_pred[:, 0]
        soe_pred_values = y_pred[:, 1]
        
        soc_soe_gap = np.abs(soc_pred_values - soe_pred_values).mean()
        
        print(f"\nğŸ† ç‹¬ç«‹é¢„æµ‹ç»¼åˆæ€§èƒ½:")
        print(f"    æ•´ä½“å¹³å‡RMSE: {avg_rmse:.6f} ({avg_rmse*100:.2f}%) ")
        print(f"    å¹³å‡MAE:  {avg_mae:.6f}")
        print(f"    å¹³å‡RÂ²:   {avg_r2:.6f}")
        print(f"    SOC-SOEå¹³å‡å·®è·: {soc_soe_gap:.6f}")
        
        print(f"\nğŸ“Š åˆ†å·¥å†µæ€§èƒ½:")
        print(f"    LA92å¹³å‡RMSE: {la92_rmse:.6f} ({la92_rmse*100:.2f}%) ")
        print(f"    UDDSå¹³å‡RMSE: {udds_rmse:.6f} ({udds_rmse*100:.2f}%) ")
        
        full_model_rmse = 0.020 # å‡è®¾å®Œæ•´KAN+Transformeræ¨¡å‹çš„RMSEåŸºå‡†
        performance_gap = avg_rmse - full_model_rmse
        degradation_pct = (performance_gap / full_model_rmse) * 100
        
        print(f"\nğŸ“‰ vs è”åˆé¢„æµ‹ï¼ˆå®Œæ•´æ¨¡å‹ï¼‰:")
        print(f"    å®Œæ•´æ¨¡å‹RMSE: {full_model_rmse:.6f}")
        print(f"    ç‹¬ç«‹é¢„æµ‹RMSE: {avg_rmse:.6f}")
        print(f"    æ€§èƒ½ä¸‹é™: {performance_gap:.6f} ({degradation_pct:.1f}%)")
        print(f"    è¯æ˜: SOC-SOEè”åˆé¢„æµ‹è´¡çŒ®äº†{degradation_pct:.1f}%çš„æ€§èƒ½æå‡ï¼")
        print(f"    ğŸ”¬ ç§‘å­¦å‘ç°:")
        print(f"       - ä¿¡æ¯å…±äº«çš„ä»·å€¼")
        print(f"       - ç‰©ç†è€¦åˆçº¦æŸçš„å¿…è¦æ€§")
        print(f"       - è”åˆå­¦ä¹  vs ç‹¬ç«‹å­¦ä¹ çš„ä¼˜åŠ¿")
        
        plt.show()
        print("="*80)
    
    def on_train_epoch_end(self): 
        self.current_epoch_num += 1
    
    def configure_optimizers(self):
        # ä¸¤ä¸ªæ¨¡å‹çš„å‚æ•°ä¸€èµ·ä¼˜åŒ–
        params = list(self.soc_model.parameters()) + list(self.soe_model.parameters())
        
        optimizer = torch.optim.AdamW(
            params, 
            lr=self.hparams.lr, 
            weight_decay=self.hparams.weight_decay
        )
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=15,
            min_lr=self.hparams.lr * 0.001
        )
        
        return {
            "optimizer": optimizer, 
            "lr_scheduler": {"scheduler": scheduler, "monitor": "val_loss"}
        }


class MemoryCleanupCallback(pl.Callback):
    """å†…å­˜æ¸…ç†å›è°ƒ"""
    def on_train_epoch_end(self, trainer, pl_module): 
        gc.collect()
        torch.cuda.empty_cache()


def main():
    parser = argparse.ArgumentParser(description='æ¶ˆèå®éªŒ A4.1: SOC-SOEç‹¬ç«‹é¢„æµ‹')
    
    parser.add_argument('--train_paths', type=str, nargs='+', default=[
        r"C:\25degC training\03-18-17_02.17 25degC_Cycle_1_Pan18650PF.csv",
        r"C:\25degC training\03-19-17_03.25 25degC_Cycle_2_Pan18650PF.csv",
        r"C:\25degC training\03-19-17_09.07 25degC_Cycle_3_Pan18650PF.csv",
        r"C:\25degC training\03-19-17_14.31 25degC_Cycle_4_Pan18650PF.csv",
        r"C:\25degC training\03-20-17_01.43 25degC_US06_Pan18650PF.csv",
        r"C:\25degC training\03-20-17_05.56 25degC_HWFTa_Pan18650PF.csv",
        r"C:\25degC testing\03-21-17_16.27 25degC_NN_Pan18650PF.csv"
    ])
    parser.add_argument('--test_paths', type=str, nargs='+', default=[
        r"C:\25degC training\03-21-17_09.38 25degC_LA92_Pan18650PF.csv",
        r"C:\25degC testing\03-21-17_00.29 25degC_UDDS_Pan18650PF.csv"
    ])
    parser.add_argument('--result_dir', type=str, default='ablation_A4_1_independent_MinMaxScaler_results')
    parser.add_argument('--output_features', type=str, default='SOC,SOE')
    parser.add_argument('--window_size', type=int, default=32)
    parser.add_argument('--overlap_ratio', type=float, default=0.5)
    parser.add_argument('--temperature', type=float, default=25.0)
    
    parser.add_argument('--n_layers', type=int, default=5)
    parser.add_argument('--num_heads', type=int, default=16)
    parser.add_argument('--hidden_space', type=int, default=128)
    parser.add_argument('--embed_dim', type=int, default=128)
    parser.add_argument('--dropout', type=float, default=0.20)
    parser.add_argument('--weight_decay', type=float, default=0.0006)
    parser.add_argument('--noise_factor', type=float, default=0.004)
    parser.add_argument('--electrochemical_weight', type=float, default=0.025)
    parser.add_argument('--grid_size', type=int, default=24)
    parser.add_argument('--num_epochs', type=int, default=200)
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--patience', type=int, default=30)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--lr', type=float, default=0.0006)
    parser.add_argument('--num_workers', type=int, default=0)
    parser.add_argument('--gradient_accumulation_steps', type=int, default=2)
    parser.add_argument('--train_val_split_ratio', type=float, default=0.93, help='Ratio for training set split from total training data.') # æ–°å¢ï¼šè®­ç»ƒé›†ä¸éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹
    parser.add_argument('--ckpt_path', type=str, default=None, help='Path to a pre-trained checkpoint to load for testing') # æ–°å¢ï¼šæ£€æŸ¥ç‚¹è·¯å¾„
    
    args = parser.parse_args()
    args.output_features = [item.strip() for item in args.output_features.split(',')]
    # os.makedirs(args.result_dir, exist_ok=True) # ç§»é™¤æ­¤è¡Œï¼Œå°†åœ¨loggerä¸­å¤„ç†
    
    print("="*80)
    print("ğŸ“Š === æ¶ˆèå®éªŒ A4.1: SOC-SOEç‹¬ç«‹é¢„æµ‹ - 25Â°C (MinMaxScaler) ===") # æ›´æ–°æ‰“å°ä¿¡æ¯
    print("="*80)
    print("ğŸ”¬ ç‰¹å¾é…ç½®:")
    print("   âœ… ä½¿ç”¨æ‰€æœ‰46ä¸ªç”µåŒ–å­¦ç‰¹å¾")
    print("   ğŸ”„ æ•°æ®æ ‡å‡†åŒ–ï¼šå…¨å±€MinMaxScaler")
    print("")
    print("ğŸ¯ å®éªŒç›®çš„:")
    print("   1. éªŒè¯SOC-SOEè”åˆé¢„æµ‹çš„ä»·å€¼")
    print("   2. é‡åŒ–ä¿¡æ¯å…±äº«å’Œç‰©ç†è€¦åˆçº¦æŸçš„è´¡çŒ®")
    print("")
    print("ğŸ“‰ ç§»é™¤çš„ç»„ä»¶:")
    print("   âŒ SOC-SOEè”åˆé¢„æµ‹ï¼ˆå•ä¸€æ¨¡å‹åŒæ—¶è¾“å‡º2ä¸ªå€¼ï¼‰")
    print("   âŒ SOC-SOEè€¦åˆçº¦æŸ (|SOE - SOC| < 0.12)")
    print("   âŒ å…±äº«ç‰¹å¾ç¼–ç å™¨")
    print("")
    print("ğŸ”„ æ›¿æ¢æ–¹æ¡ˆ:")
    print("   ä¸¤ä¸ªå®Œå…¨ç‹¬ç«‹çš„æ¨¡å‹ï¼š")
    print("   - SOCæ¨¡å‹ï¼šç‹¬ç«‹ç¼–ç å™¨ + Transformer + é¢„æµ‹å¤´ â†’ SOC")
    print("   - SOEæ¨¡å‹ï¼šç‹¬ç«‹ç¼–ç å™¨ + Transformer + é¢„æµ‹å¤´ â†’ SOE")
    print("")
    print("âœ… ä¿ç•™çš„ç»„ä»¶:")
    print("   âœ“ ç›¸åŒçš„æ¶æ„å¤æ‚åº¦ï¼ˆæ¯ä¸ªæ¨¡å‹ï¼‰")
    print("   âœ“ ç›¸åŒçš„è®­ç»ƒç­–ç•¥")
    print("   âœ“ ç›¸åŒçš„è¶…å‚æ•°")
    print("\nğŸ“Š é¢„æœŸæ€§èƒ½ä¸‹é™: 8-12% (ç›¸æ¯”å®Œæ•´æ¨¡å‹)")
    print("   ç›®çš„: éªŒè¯è”åˆé¢„æµ‹å’Œç‰©ç†è€¦åˆçš„ä»·å€¼")
    print("="*80)
    
    try:
        pl.seed_everything(args.seed, workers=True)
        datamodule = Electrochemical46FeaturesKANDataModule(args) # ä½¿ç”¨æ–°çš„KANæ•°æ®æ¨¡å—
        datamodule.setup(stage='fit') # æ˜¾å¼è°ƒç”¨setupï¼Œç¡®ä¿scaleræ‹Ÿåˆ
        
        model = IndependentSOCSOELightningModule(args)
        
        # æ˜¾å¼é…ç½®loggerï¼Œä½¿å…¶ä¿å­˜åˆ°args.result_dir
        from pytorch_lightning.loggers import TensorBoardLogger
        logger = TensorBoardLogger(save_dir=args.result_dir, name='', version='') # nameä¸ºç©ºï¼Œversionä¸ºç©ºï¼Œç›´æ¥ä¿å­˜åˆ°save_dir

        checkpoint_callback = ModelCheckpoint(
            dirpath=os.path.join(args.result_dir, 'checkpoints'), # å°†æ£€æŸ¥ç‚¹ä¿å­˜åˆ°å­æ–‡ä»¶å¤¹
            filename='ablation-independent-{epoch:02d}-{val_loss:.6f}',
            save_top_k=1, monitor='val_loss', mode='min'
        )
        early_stop_callback = EarlyStopping(
            monitor='val_loss', patience=args.patience, mode='min'
        )
        lr_monitor = LearningRateMonitor(logging_interval='epoch')
        
        trainer = pl.Trainer(
            max_epochs=args.num_epochs, 
            accelerator='auto', 
            devices=1,
            callbacks=[checkpoint_callback, early_stop_callback, lr_monitor, MemoryCleanupCallback()],
            precision='32',
            gradient_clip_val=0.5,
            accumulate_grad_batches=args.gradient_accumulation_steps,
            logger=logger # ä¼ å…¥é…ç½®å¥½çš„logger
        )
        
        print(f"\nğŸš€ å¼€å§‹ç‹¬ç«‹é¢„æµ‹è®­ç»ƒ...")
        print(f"âš ï¸  æ³¨æ„ï¼šä½¿ç”¨ä¸¤ä¸ªå®Œå…¨ç‹¬ç«‹çš„æ¨¡å‹ï¼Œå‚æ•°é‡ç¿»å€ï¼")
        if args.ckpt_path:
            print(f"   è·³è¿‡è®­ç»ƒï¼Œä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹: {args.ckpt_path}")
            model = type(model).load_from_checkpoint(args.ckpt_path, hparams=args)
            trainer.test(model, datamodule=datamodule)
        else:
            trainer.fit(model, datamodule)
            print(f"\nğŸ“Š æµ‹è¯•ç‹¬ç«‹é¢„æµ‹æ¨¡å‹...")
            trainer.test(model, datamodule=datamodule, ckpt_path='best')
        
        print(f"\nâœ… æ¶ˆèå®éªŒ A4.1 å®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.result_dir}") # æ–°å¢ï¼šæ‰“å°ç»“æœä¿å­˜è·¯å¾„
        
    except Exception as e:
        print(f"âŒ å®éªŒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == '__main__':
    main()
