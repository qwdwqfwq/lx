import argparse
import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import TensorDataset, DataLoader, random_split
from sklearn.preprocessing import StandardScaler
import pytorch_lightning as pl

# å¯¼å…¥åŸå§‹çš„ç‰¹å¾å·¥ç¨‹æ¨¡å—
from electrochemical_features import ElectrochemicalFeatureEngineer

def process_single_dataframe_for_46features(
    data_df: pd.DataFrame, 
    temperature, 
    engineer: ElectrochemicalFeatureEngineer, 
    window_size=32, 
    overlap_ratio=0.5, 
    is_test_set=False,
    feature_scalers=None
):
    """
    å¯¹å•ä¸ªåŸå§‹DataFrameè¿›è¡Œ46ä¸ªç”µåŒ–å­¦ç‰¹å¾å·¥ç¨‹ï¼Œå¹¶è¿›è¡Œçª—å£åŒ–å¤„ç†ã€‚
    æ”¯æŒå…¨å±€æ ‡å‡†åŒ–å’Œæµ‹è¯•é›†æ­¥é•¿ä¸º1ã€‚
    """
    all_features_list, all_targets_list, all_original_end_indices = [], [], []
    
    # ç¡®å®šæ­¥é•¿
    step_size = 1 if is_test_set else max(1, int(window_size * (1 - overlap_ratio)))

    try:
        if data_df.empty:
            print(f"   âš ï¸ è¾“å…¥DataFrameä¸ºç©ºï¼Œè·³è¿‡ã€‚")
            return np.array([]), np.array([]), np.array([]) # ç¡®ä¿è¿”å›3ä¸ªç©ºæ•°ç»„

        # --- ç‰¹å¾å·¥ç¨‹ ---
        features_df = engineer.create_electrochemical_features(data_df, temperature)
        targets_df = data_df[['SOC', 'SOE']]

        if features_df.empty or targets_df.empty:
            print(f"   âš ï¸ ç‰¹å¾æˆ–ç›®æ ‡ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
            return np.array([]), np.array([]), np.array([]) # ç¡®ä¿è¿”å›3ä¸ªç©ºæ•°ç»„

        # --- ç‰¹å¾æ ‡å‡†åŒ– ---
        if feature_scalers:
            if is_test_set is False and feature_scalers == 'fit_only':
                scaled_features = features_df.values
            else: # åº”ç”¨æ ‡å‡†åŒ–
                if not hasattr(feature_scalers, 'transform'):
                    raise ValueError("Provided feature_scalers must be a fitted StandardScaler instance.")
                scaled_features = feature_scalers.transform(features_df.values)
        else:
            # æ²¡æœ‰æä¾› scalerï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ– (ä»…ç”¨äºæ‹Ÿåˆé˜¶æ®µè·å–åŸå§‹æ•°æ®)
            scaled_features = features_df.values

        # --- ç›®æ ‡ï¼šä¸è¿›è¡Œæ ‡å‡†åŒ– (ä¿æŒåŸå§‹ç‰©ç†å€¼) ---
        scaled_targets = targets_df.values # ç›´æ¥ä½¿ç”¨åŸå§‹ç‰©ç†å€¼

        # --- æ»‘åŠ¨çª—å£å¤„ç† ---
        max_i = len(scaled_features) - window_size
        if max_i < 0:
            print(f"   âš ï¸ æ•°æ®é•¿åº¦ä¸è¶³ä»¥åˆ›å»ºçª—å£ï¼Œè·³è¿‡ã€‚")
            return np.array([]), np.array([]), np.array([]) # ç¡®ä¿è¿”å›3ä¸ªç©ºæ•°ç»„

        for i in range(0, max_i + 1, step_size):
            window_features = scaled_features[i : i + window_size]
            target_value = scaled_targets[i + window_size - 1] # é¢„æµ‹çª—å£çš„æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼Œç°åœ¨æ˜¯åŸå§‹ç‰©ç†å€¼

            if not (np.isnan(window_features).any() or np.isnan(target_value).any()):
                all_features_list.append(window_features)
                all_targets_list.append(target_value)
                all_original_end_indices.append(i + window_size - 1) # è®°å½•åŸå§‹æ—¶é—´æ­¥ç´¢å¼•

    except Exception as e:
        print(f"   âŒ å¤„ç†DataFrameæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return np.array([]), np.array([]), np.array([]) # ç¡®ä¿è¿”å›3ä¸ªç©ºæ•°ç»„

    if not all_features_list:
        print(f"   âš ï¸ æœªèƒ½ä»æä¾›çš„DataFrameä¸­åŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ã€‚")
        return np.array([]), np.array([]), np.array([]) # ç¡®ä¿è¿”å›3ä¸ªç©ºæ•°ç»„
    
    # ğŸš¨ [ä¼˜åŒ–] ä½¿ç”¨ np.stack æ›¿ä»£ np.arrayï¼Œå¯èƒ½æ›´é«˜æ•ˆ
    return np.stack(all_features_list, dtype=np.float32), \
           np.stack(all_targets_list, dtype=np.float32), \
           np.stack(all_original_end_indices, dtype=np.int64) # ç´¢å¼•ç”¨int64ç¡®ä¿ä¸ä¼šæº¢å‡º

def _load_data_from_file_paths(
    csv_paths,
    temperature,
    engineer,
    window_size,
    overlap_ratio,
    is_test_set,
    scaler,
):
    feat_list, tgt_list, idx_list = [], [], []
    for file_path in csv_paths:
        try:
            df = pd.read_csv(file_path)
        except Exception as e:
            print(f"   åŠ è½½æ–‡ä»¶ {file_path.split('/')[-1]} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
        f, t, idx = process_single_dataframe_for_46features(
            df,
            temperature,
            engineer,
            window_size=window_size,
            overlap_ratio=overlap_ratio,
            is_test_set=is_test_set,
            feature_scalers=scaler,
        )
        if f.size == 0:
            continue
        feat_list.append(f)
        tgt_list.append(t)
        idx_list.append(idx)

    if not feat_list:
        return np.array([]), np.array([]), np.array([])

    return (
        np.concatenate(feat_list, axis=0),
        np.concatenate(tgt_list, axis=0),
        np.concatenate(idx_list, axis=0),
    )


class Electrochemical46FeaturesDataModule(pl.LightningDataModule):
    """
    ç”µåŒ–å­¦46ç‰¹å¾æ•°æ®æ¨¡å—ï¼Œæ”¯æŒå…¨å±€æ ‡å‡†åŒ–å’Œæµ‹è¯•é›†æ­¥é•¿ä¸º1ã€‚
    """
    def __init__(self, hparams):
        super().__init__()
        self.save_hyperparameters(hparams)
        self.feature_scaler = None # ç”¨äºå­˜å‚¨æ‹Ÿåˆåçš„46ç‰¹å¾å…¨å±€æ ‡å‡†åŒ–å™¨
        self.train_dataset = None
        self.val_dataset = None
        self.test_datasets = []
        self.engineer = ElectrochemicalFeatureEngineer() # åªåˆå§‹åŒ–ä¸€æ¬¡ç‰¹å¾å·¥ç¨‹å¸ˆ

    def prepare_data(self):
        # åœ¨å¤šGPUåœºæ™¯ä¸‹ï¼Œæ­¤æ–¹æ³•åªåœ¨ä¸€ä¸ªè¿›ç¨‹ä¸Šè¿è¡Œï¼Œé€‚åˆä¸‹è½½æ•°æ®ç­‰
        pass

    def setup(self, stage=None):
        print(f"ğŸ§ª å¼€å§‹è®¾ç½®ç”µåŒ–å­¦46ç‰¹å¾æ•°æ®æ¨¡å— (æ¸©åº¦: {self.hparams.temperature}Â°C, é˜¶æ®µ: {stage})")

        # --- 1. åŸºäºæ–‡ä»¶è¿›è¡Œè®­ç»ƒé›†å’ŒéªŒè¯é›†çš„åˆ’åˆ† ---
        # ç¡®ä¿è®­ç»ƒæ–‡ä»¶åˆ—è¡¨æ˜¯å¯é¢„æµ‹çš„é¡ºåº
        train_files_shuffled = sorted(self.hparams.train_paths) # ä¿æŒé¡ºåºä¸€è‡´æ€§
        total_train_files = len(train_files_shuffled)
        
        # è®¡ç®—è®­ç»ƒæ–‡ä»¶æ•°é‡ï¼Œè‡³å°‘ä¿ç•™1ä¸ªæ–‡ä»¶ç”¨äºè®­ç»ƒ
        num_train_for_split = max(1, int(total_train_files * self.hparams.train_val_split_ratio))
        if num_train_for_split == total_train_files: # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªéªŒè¯æ–‡ä»¶ï¼Œå¦‚æœæ–‡ä»¶æ•°è¶³å¤Ÿ
            if total_train_files > 1:
                num_train_for_split = total_train_files - 1
            else:
                # å¦‚æœåªæœ‰ä¸€ä¸ªæ–‡ä»¶ï¼Œå°±å…¨éƒ¨ç”¨äºè®­ç»ƒï¼ŒéªŒè¯é›†ä¸ºç©º
                num_train_for_split = total_train_files

        train_files = train_files_shuffled[:num_train_for_split]
        val_files = train_files_shuffled[num_train_for_split:]

        if not train_files:
            raise ValueError("æ²¡æœ‰è®­ç»ƒæ–‡ä»¶å¯ç”¨ã€‚è¯·æ£€æŸ¥train_pathsæˆ–train_val_split_ratioã€‚")
        # å³ä½¿val_filesä¸ºç©ºï¼Œä¹Ÿå…è®¸ç»§ç»­ï¼Œå› ä¸ºæœ‰äº›åœºæ™¯å¯èƒ½ä¸éœ€è¦éªŒè¯é›†
        if not val_files:
            print("   è­¦å‘Š: æ²¡æœ‰éªŒè¯æ–‡ä»¶å¯ç”¨ã€‚æ¨¡å‹å°†åœ¨æ²¡æœ‰ç‹¬ç«‹éªŒè¯é›†çš„æƒ…å†µä¸‹è¿›è¡Œè®­ç»ƒã€‚")

        print(f"   æ–‡ä»¶åˆ’åˆ†ï¼š è®­ç»ƒæ–‡ä»¶æ•°={len(train_files)}, éªŒè¯æ–‡ä»¶æ•°={len(val_files)}")

        # --- 2. æ‹Ÿåˆç‰¹å¾scaler (ä»…åœ¨è®­ç»ƒæ–‡ä»¶ä¸Š) ---
        if stage == "fit" and self.feature_scaler is None:
            print("   ğŸ“Š æ”¶é›†è®­ç»ƒæ–‡ä»¶æ•°æ®ä»¥æ‹Ÿåˆå…¨å±€StandardScaler...")
            # ä»…åŠ è½½è®­ç»ƒæ–‡ä»¶çš„æ•°æ®ç”¨äºScaleræ‹Ÿåˆ
            raw_train_features_for_scaler, _, _ = _load_data_from_file_paths(
                train_files,
                self.hparams.temperature,
                self.engineer,
                self.hparams.window_size,
                self.hparams.overlap_ratio,
                is_test_set=False,
                scaler="fit_only", # æ ‡è®°ä¸ºåªç”¨äºæ‹Ÿåˆ
            )
            if raw_train_features_for_scaler.size == 0:
                raise ValueError("è®­ç»ƒé›†æ•°æ®ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ ‡å‡†åŒ–å™¨æ‹Ÿåˆã€‚è¯·æ£€æŸ¥train_filesã€‚")
            
            # å°†æ‰€æœ‰çª—å£çš„ç‰¹å¾å±•å¹³è¿›è¡Œæ‹Ÿåˆ
            flattened_train_features = raw_train_features_for_scaler.reshape(-1, raw_train_features_for_scaler.shape[-1])
            self.feature_scaler = StandardScaler()
            self.feature_scaler.fit(flattened_train_features)
            print("   âœ… å…¨å±€ç‰¹å¾StandardScaleræ‹Ÿåˆå®Œæˆï¼ˆä»…åŸºäºè®­ç»ƒæ–‡ä»¶ï¼‰ã€‚")

        # --- 3. åŠ è½½å’Œæ ‡å‡†åŒ–è®­ç»ƒæ•°æ® ---
        print("   ğŸ“ åŠ è½½å’Œå¤„ç†è®­ç»ƒæ•°æ®...")
        train_features, train_targets, train_original_end_indices = _load_data_from_file_paths(
            train_files,
            self.hparams.temperature,
            self.engineer,
            self.hparams.window_size,
            self.hparams.overlap_ratio,
            is_test_set=False,
            feature_scalers=self.feature_scaler, # åº”ç”¨æ‹Ÿåˆå¥½çš„Scaler
        )
        if train_features.size == 0:
            raise ValueError("è®­ç»ƒæ•°æ®åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥train_filesã€‚")

        X_train_tensor = torch.from_numpy(train_features).float()
        y_train_tensor = torch.from_numpy(train_targets).float()
        train_original_end_indices_tensor = torch.from_numpy(train_original_end_indices).long()
        self.train_dataset = TensorDataset(X_train_tensor, y_train_tensor, train_original_end_indices_tensor)
        print(f"   è®­ç»ƒé›†æ ·æœ¬æ•°é‡: {len(self.train_dataset):,}")

        # --- 4. åŠ è½½å’Œæ ‡å‡†åŒ–éªŒè¯æ•°æ® ---
        print("   ğŸ“ åŠ è½½å’Œå¤„ç†éªŒè¯æ•°æ®...")
        if val_files:
            val_features, val_targets, val_original_end_indices = _load_data_from_file_paths(
                val_files,
                self.hparams.temperature,
                self.engineer,
                self.hparams.window_size,
                self.hparams.overlap_ratio,
                is_test_set=False,
                feature_scalers=self.feature_scaler, # åº”ç”¨æ‹Ÿåˆå¥½çš„Scaler
            )
            if val_features.size == 0:
                print("   âš ï¸ éªŒè¯æ•°æ®åŠ è½½å¤±è´¥æˆ–ä¸ºç©ºã€‚éªŒè¯é›†å°†è¢«è®¾ç½®ä¸ºç©ºã€‚")
                self.val_dataset = None
            else:
                X_val_tensor = torch.from_numpy(val_features).float()
                y_val_tensor = torch.from_numpy(val_targets).float()
                val_original_end_indices_tensor = torch.from_numpy(val_original_end_indices).long()
                self.val_dataset = TensorDataset(X_val_tensor, y_val_tensor, val_original_end_indices_tensor)
                print(f"   éªŒè¯é›†æ ·æœ¬æ•°é‡: {len(self.val_dataset):,}")
        else:
            self.val_dataset = None
            print("   æœªè®¾ç½®éªŒè¯é›†ï¼Œè·³è¿‡éªŒè¯æ•°æ®åŠ è½½ã€‚")

        # --- 5. å¤„ç†æµ‹è¯•æ•°æ® (æ­¥é•¿=1) ---
        print("   ğŸ“ åŠ è½½å’Œå¤„ç†æµ‹è¯•æ•°æ® (æ­¥é•¿=1)...")
        self.test_datasets = []
        if self.hparams.test_paths:
            for file_path in self.hparams.test_paths:
                df = pd.read_csv(file_path)
                test_features, test_targets, test_original_end_indices = process_single_dataframe_for_46features(
                    df,
                    self.hparams.temperature,
                    self.engineer,
                    window_size=self.hparams.window_size,
                    overlap_ratio=self.hparams.overlap_ratio,
                    is_test_set=True,
                    feature_scalers=self.feature_scaler, # åº”ç”¨æ‹Ÿåˆå¥½çš„Scaler
                )
                if test_features.size == 0:
                    print(f"   âš ï¸ æµ‹è¯•æ–‡ä»¶ {file_path.split('/')[-1]} åŠ è½½å¤±è´¥æˆ–ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
                    continue

                test_dataset = TensorDataset(
                    torch.from_numpy(test_features).float(),
                    torch.from_numpy(test_targets).float(),
                    torch.from_numpy(test_original_end_indices).long()
                )
                self.test_datasets.append(test_dataset)
            if not self.test_datasets:
                print("   æµ‹è¯•æ•°æ®ä¸ºç©ºï¼Œæœªåˆ›å»ºæµ‹è¯•DataLoaderã€‚")
        else:
            print("   æœªæä¾›æµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼Œè·³è¿‡æµ‹è¯•æ•°æ®åŠ è½½ã€‚")

        print(f"âœ… ç”µåŒ–å­¦46ç‰¹å¾æ•°æ®æ¨¡å—è®¾ç½®å®Œæˆï¼Œè®­ç»ƒç‰¹å¾æ•°: {train_features.shape[-1]}")

    @property
    def scaler(self):
        """æä¾›å¯¹å·²æ‹Ÿåˆçš„ç‰¹å¾æ ‡å‡†åŒ–å™¨çš„è®¿é—®"""
        if self.feature_scaler is None:
            raise RuntimeError("ç‰¹å¾æ ‡å‡†åŒ–å™¨å°šæœªæ‹Ÿåˆã€‚è¯·å…ˆè¿è¡Œ setup(stage='fit')ã€‚")
        return {'features': self.feature_scaler}

    def train_dataloader(self):
        return DataLoader(
            self.train_dataset,
            batch_size=self.hparams.batch_size,
            shuffle=True,
            num_workers=self.hparams.num_workers,
            pin_memory=True,
            drop_last=True
        )

    def val_dataloader(self):
        if self.val_dataset is None:
            return None # å¦‚æœæ²¡æœ‰éªŒè¯é›†ï¼Œè¿”å›None
        return DataLoader(
            self.val_dataset,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            pin_memory=True,
            drop_last=True
        )

    def test_dataloader(self):
        return [DataLoader(
            ds,
            batch_size=self.hparams.batch_size,
            num_workers=self.hparams.num_workers,
            pin_memory=True
        ) for ds in self.test_datasets]

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    parser = argparse.ArgumentParser(description='Electrochemical 46 Features DataModule Test')
    parser.add_argument('--train_paths', type=str, nargs='+', default=[
        r"C:degC training-18-17_02.17 25degC_Cycle_1_Pan18650PF.csv",
        r"C:degC training-19-17_03.25 25degC_Cycle_2_Pan18650PF.csv",
    ])
    parser.add_argument('--test_paths', type=str, nargs='+', default=[
        r"C:degC testing-21-17_00.29 25degC_UDDS_Pan18650PF.csv",
    ])
    parser.add_argument('--temperature', type=float, default=25.0)
    parser.add_argument('--window_size', type=int, default=32)
    parser.add_argument('--overlap_ratio', type=float, default=0.5)
    parser.add_argument('--output_features', type=str, default='SOC,SOE')
    parser.add_argument('--batch_size', type=int, default=32)
    parser.add_argument('--num_workers', type=int, default=0)
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--train_val_split_ratio', type=float, default=0.93)
    
    args = parser.parse_args()
    args.output_features = [item.strip() for item in args.output_features.split(',')]

    print("ğŸ”¬ æµ‹è¯• Electrochemical46FeaturesDataModule...")
    
    datamodule = Electrochemical46FeaturesDataModule(args)
    datamodule.setup(stage='fit')

    train_loader = datamodule.train_dataloader()
    val_loader = datamodule.val_dataloader()
    test_loaders = datamodule.test_dataloader()

    print(f"è®­ç»ƒæ•°æ®åŠ è½½å™¨æ‰¹æ¬¡æ•°é‡: {len(train_loader)}")
    print(f"éªŒè¯æ•°æ®åŠ è½½å™¨æ‰¹æ¬¡æ•°é‡: {len(val_loader)}")
    print(f"æµ‹è¯•æ•°æ®åŠ è½½å™¨æ•°é‡: {len(test_loaders)}")

    if train_loader:
        for batch_idx, batch in enumerate(train_loader):
            x, y, original_indices = batch
            print(f"è®­ç»ƒæ‰¹æ¬¡ {batch_idx}: Xå½¢çŠ¶={x.shape}, Yå½¢çŠ¶={y.shape}, åŸå§‹ç´¢å¼•å½¢çŠ¶={original_indices.shape}")
            break
    
    if test_loaders:
        for ds_idx, test_loader in enumerate(test_loaders):
            for batch_idx, batch in enumerate(test_loader):
                x, y, original_indices = batch
                print(f"æµ‹è¯•é›† {ds_idx} æ‰¹æ¬¡ {batch_idx}: Xå½¢çŠ¶={x.shape}, Yå½¢çŠ¶={y.shape}, åŸå§‹ç´¢å¼•å½¢çŠ¶={original_indices.shape}")
                break
            break

    print("âœ… Electrochemical46FeaturesDataModule æµ‹è¯•å®Œæˆã€‚")
