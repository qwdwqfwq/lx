
"""
25Â°CçœŸæ­£SOTAè®­ç»ƒè„šæœ¬ - ä¿æŒUDDSä¼˜åŠ¿ï¼Œå¤§å¹…æå‡NN
æ­£ç¡®çš„ä¼˜åŒ–æ€è·¯ï¼šä¸æ˜¯å¹³è¡¡ï¼Œè€Œæ˜¯åŒé‡æå‡

ğŸ¯ æ­£ç¡®ç­–ç•¥:
1. ä¿æŒUDDSçš„ä¼˜ç§€æ€§èƒ½ (0.012å·¦å³)
2. ä¸“é—¨é’ˆå¯¹NNå·¥å†µçš„æ·±åº¦ä¼˜åŒ–
3. å·¥å†µç‰¹å®šçš„æ¶æ„åˆ†æ”¯
4. è‡ªé€‚åº”æŸå¤±åŠ æƒ
5. ç›®æ ‡: UDDSä¿æŒ0.012ï¼ŒNNé™åˆ°0.015ï¼Œæ•´ä½“çªç ´0.018
"""

import argparse
import os
import gc
import time
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import pytorch_lightning as pl
from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, Callback, LearningRateMonitor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
from scipy.signal import savgol_filter

# ä½¿ç”¨æ–°çš„ç”µåŒ–å­¦ç‰¹å¾å·¥ç¨‹
from electrochemical_features import ElectrochemicalFeatureEngineer, create_electrochemical_dataset
from model_code_lightning import setup_chinese_font
# from electrochemical_46features_datamodule import Electrochemical46FeaturesDataModule # ç§»é™¤æ—§çš„å¯¼å…¥
from electrochemical_46features_datamodule import Electrochemical46FeaturesDataModule # å¯¼å…¥ä½¿ç”¨StandardScalerçš„æ•°æ®æ¨¡å—

setup_chinese_font()


def setup_sci_style():
    """è®¾ç½®SCIè®ºæ–‡çº§åˆ«çš„matplotlibæ ·å¼ - å‚ç…§ç”¨æˆ·æä¾›çš„å›¾è¡¨"""
    plt.style.use('default')
    
    # å­—ä½“è®¾ç½® - å‚ç…§å‚è€ƒå›¾
    plt.rcParams['font.family'] = ['Times New Roman', 'Arial', 'SimHei', 'DejaVu Sans']
    plt.rcParams['font.size'] = 11          # å‚è€ƒå›¾çš„å­—ä½“å¤§å°
    plt.rcParams['axes.titlesize'] = 12     # å­å›¾æ ‡é¢˜
    plt.rcParams['axes.labelsize'] = 11     # è½´æ ‡ç­¾
    plt.rcParams['xtick.labelsize'] = 10    # åˆ»åº¦æ ‡ç­¾
    plt.rcParams['ytick.labelsize'] = 10
    plt.rcParams['legend.fontsize'] = 10    # å›¾ä¾‹
    plt.rcParams['axes.unicode_minus'] = False
    
    # SCIå®¡ç¾è®¾ç½® - æ¨¡ä»¿å‚è€ƒå›¾
    plt.rcParams['axes.linewidth'] = 1.0    # æ›´ç»†çš„è¾¹æ¡†
    plt.rcParams['xtick.major.width'] = 0.8
    plt.rcParams['ytick.major.width'] = 0.8
    plt.rcParams['xtick.minor.width'] = 0.6
    plt.rcParams['ytick.minor.width'] = 0.6
    plt.rcParams['grid.linewidth'] = 0.5    # éå¸¸ç»†çš„ç½‘æ ¼çº¿
    plt.rcParams['grid.alpha'] = 0.3        # é€æ˜ç½‘æ ¼
    plt.rcParams['lines.linewidth'] = 1.8   # å‚è€ƒå›¾çš„çº¿å®½
    
    # å‚è€ƒå›¾çš„é…è‰²æ–¹æ¡ˆ
    plt.rcParams['axes.prop_cycle'] = plt.cycler('color', 
        ['#0072BD', '#D95319', '#4DBEEE', '#A2142F'])  # è“è‰²ã€æ©™çº¢è‰²ç³»


def smooth_data(data, window_length=9, polyorder=2):
    """æ•°æ®å¹³æ»‘å¤„ç†ï¼Œæå‡SCIå›¾è¡¨è´¨é‡"""
    if len(data) < window_length:
        return data
    
    # ç¡®ä¿window_lengthæ˜¯å¥‡æ•°
    if window_length % 2 == 0:
        window_length += 1
    
    # ç¡®ä¿polyorder < window_length
    polyorder = min(polyorder, window_length - 1)
    
    try:
        return savgol_filter(data, window_length, polyorder)
    except:
        return data


class TrueSOTAConstraintLayer(nn.Module):
    """çœŸæ­£SOTAçš„ç‰©ç†çº¦æŸå±‚"""
    
    def __init__(self, num_outputs, temperature=None, feature_scaler=None): # ç§»é™¤ target_scaler å‚æ•°
        super().__init__()
        self.num_outputs = num_outputs
        self.temperature = temperature
        self.feature_scaler = feature_scaler # å­˜å‚¨ç‰¹å¾æ ‡å‡†åŒ–å™¨ (ç”¨äºè¾“å…¥ç‰¹å¾)
        
        # å·¥å†µè‡ªé€‚åº”çº¦æŸå¼ºåº¦ (æ¢å¤ä¸ºå›ºå®šå€¼)
        self.nn_constraint_strength = 1.0   
        self.udds_constraint_strength = 1.0  
        
        self.soc_soe_coupling = 0.6 # ä» 0.8 è¿›ä¸€æ­¥è°ƒæ•´ä¸º 0.6ï¼Œç»§ç»­å‡å¼±è€¦åˆå¼ºåº¦
        self.electrochemical_weight = 0.055 # è¿™ä¸ªå‚æ•°ä¸åœ¨çº¦æŸå±‚ä½¿ç”¨ï¼Œä¿æŒä¸å˜
        
        if temperature is not None:
            self.temp_constraint = 1.0 # æ¢å¤ä¸ºå›ºå®šå€¼1.0
        else:
            self.temp_constraint = None
            
    def forward(self, predictions, inputs): # predictions æ­¤æ—¶åº”æ˜¯èåˆåçš„ logits
        # ğŸš¨ [å…³é”®ä¿®æ”¹] åœ¨è¿™é‡Œå¯¹ predictions è¿›è¡Œ Sigmoid æ¿€æ´»ï¼Œå¹¶ç¡®ä¿å…¶åœ¨ 0-1 èŒƒå›´
        # ä»¥å‰åœ¨ forward é‡Œåš Tanh æ¿€æ´»å’Œç¼©æ”¾ï¼Œç°åœ¨ç›´æ¥ç”¨ Sigmoid
        predictions_physical = torch.sigmoid(predictions)
        
        # ç¡®ä¿ predictions_physical åœ¨ 0-1 èŒƒå›´ï¼Œå¹¶è¿›è¡Œç‰©ç†çº¦æŸ
        constrained_physical = predictions_physical.clone()

        # å·¥å†µè‡ªé€‚åº”çº¦æŸå¼ºåº¦
        # avg_constraint_strength = (self.nn_constraint_strength + self.udds_constraint_strength) / 2 # å·²ç¦ç”¨ï¼Œä½¿ç”¨å›ºå®šå€¼

        # SOC-SOEè€¦åˆçº¦æŸ
        if self.num_outputs >= 2:
            soc_pred_physical = constrained_physical[:, :, 0] # ç‰©ç†SOC
            soe_pred_physical = constrained_physical[:, :, 1] # ç‰©ç†SOE
            
            coupling_strength = self.soc_soe_coupling # ç°åœ¨æ˜¯å›ºå®šå€¼
            
            # åœ¨ç‰©ç†å°ºåº¦ä¸Šåº”ç”¨è€¦åˆçº¦æŸ
            soe_constrained_physical = torch.minimum(soe_pred_physical, soc_pred_physical + 0.06)
            new_soe_physical = coupling_strength * soe_constrained_physical + (1 - coupling_strength) * soe_pred_physical
            
            # å°†ä¿®æ­£åçš„SOEé‡æ–°æ”¾å›ç‰©ç†é¢„æµ‹å¼ é‡ï¼Œé¿å…åŸåœ°æ“ä½œ
            constrained_physical = torch.cat([constrained_physical[:, :, 0:1], new_soe_physical.unsqueeze(-1)], dim=-1)

        # æ¸©åº¦çº¦æŸ (åœ¨ç‰©ç†å°ºåº¦ä¸Šåº”ç”¨ï¼Œå¦‚æœéœ€è¦)
        if self.temp_constraint is not None and self.temperature is not None:
            temp_factor = 1.0 if self.temperature >= 25 else 0.85 + 0.15 * (self.temperature + 20) / 45
            temp_strength = self.temp_constraint # ç°åœ¨æ˜¯å›ºå®šå€¼
            # å°†æ•´ä¸ªç‰©ç†é¢„æµ‹å€¼æ ¹æ®æ¸©åº¦å› å­è¿›è¡Œè°ƒæ•´
            constrained_physical = temp_strength * constrained_physical * temp_factor + (1 - temp_strength) * constrained_physical

        # ğŸš¨ [å…³é”®ä¿®æ”¹] åœ¨è¿”å›å‰æ·»åŠ æ˜¾å¼çš„ Clampï¼Œç¡®ä¿ä¸¥æ ¼åœ¨ 0-1 èŒƒå›´
        constrained_physical_clamped = torch.clamp(constrained_physical, 0.0, 1.0)
        return constrained_physical_clamped # è¿”å›ä¸¥æ ¼ Clamp åçš„ç‰©ç†å€¼


class TrueSOTAElectrochemicalKANTransformer(nn.Module):
    """
    çœŸæ­£SOTAçš„ç”µåŒ–å­¦KAN-Transformeræ¶æ„
    ä¿æŒUDDSä¼˜åŠ¿ï¼Œä¸“é—¨æå‡NNæ€§èƒ½
    """
    
    def __init__(self, input_dim, num_heads, num_layers, num_outputs, 
                 hidden_space, dropout_rate, embed_dim, grid_size=16,
                 temperature=None, feature_scaler=None, detector_temp_init=0.7):
        super().__init__()
        
        self.input_dim = input_dim
        self.num_outputs = num_outputs
        self.temperature = temperature
        self.feature_scaler = feature_scaler # å­˜å‚¨æ ‡å‡†åŒ–å™¨
        self.detector_temperature = nn.Parameter(torch.tensor(detector_temp_init)) # workload detector softmax temperature
        
        # === [ç­–ç•¥1] ä¿æŒæˆåŠŸçš„ç‰¹å¾ç¼–ç å™¨ ===
        # åŸºäºUDDSæˆåŠŸç»éªŒçš„ç‰¹å¾åˆ†ç»„ç¼–ç 
        
        # åŸºç¡€ç”µåŒ–å­¦ç‰¹å¾ç¼–ç å™¨ (é’ˆå¯¹UDDSä¼˜åŒ–)
        self.basic_encoder = nn.Sequential(
            nn.Linear(10, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//4, hidden_space//4),
            nn.LayerNorm(hidden_space//4)
        )
        
        # åŠ¨æ€å“åº”ç‰¹å¾ç¼–ç å™¨ (UDDSçš„å¼ºé¡¹)
        self.dynamic_encoder = nn.Sequential(
            nn.Linear(12, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//4, hidden_space//4),
            nn.LayerNorm(hidden_space//4)
        )
        
        # èƒ½é‡åº“ä»‘ç‰¹å¾ç¼–ç å™¨
        self.energy_encoder = nn.Sequential(
            nn.Linear(8, hidden_space//6),
            nn.LayerNorm(hidden_space//6),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//6, hidden_space//6),
            nn.LayerNorm(hidden_space//6)
        )
        
        # é˜»æŠ—è€åŒ–ç‰¹å¾ç¼–ç å™¨
        self.impedance_encoder = nn.Sequential(
            nn.Linear(10, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//4, hidden_space//4),
            nn.LayerNorm(hidden_space//4)
        )
        
        # æ¸©åº¦è¡¥å¿ç‰¹å¾ç¼–ç å™¨
        self.temperature_encoder = nn.Sequential(
            nn.Linear(6, hidden_space//6),
            nn.LayerNorm(hidden_space//6),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//6, hidden_space//6),
            nn.LayerNorm(hidden_space//6)
        )
        
        # === [ç­–ç•¥2] å·¥å†µæ£€æµ‹å™¨ ===
        base_encoded_dim = hidden_space//4 * 3 + hidden_space//6 * 2
        # æ–°å¢ï¼š1Dæ³¨æ„åŠ›æ± åŒ–å±‚
        self.attention_pooling = nn.Sequential(
            nn.Linear(base_encoded_dim, 1),
            nn.Softmax(dim=1) # åœ¨æ—¶é—´ç»´åº¦ä¸Šè¿›è¡ŒSoftmax
        )
        detector_input_dim = base_encoded_dim # æ³¨æ„åŠ›æ± åŒ–åç»´åº¦ä¸ç¼–ç ç»´åº¦ç›¸åŒ
        self.workload_detector = nn.Sequential(
            nn.Linear(detector_input_dim, hidden_space//2),
            nn.LayerNorm(hidden_space//2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.1),
            nn.Linear(hidden_space//2, hidden_space//4),
            nn.ReLU(),
            nn.Linear(hidden_space//4, 2)  # NN vs UDDSï¼ŒSoftmaxÆ¶forwardÂ¶
        )
        
        # === [ç­–ç•¥3] å·¥å†µç‰¹å®šçš„å¤„ç†åˆ†æ”¯ ===
        
        # UDDSåˆ†æ”¯ (ä¿æŒç®€å•æœ‰æ•ˆçš„è®¾è®¡)
        self.udds_branch = nn.Sequential(
            nn.Linear(base_encoded_dim, hidden_space),
            nn.LayerNorm(hidden_space),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.10)
        )
        
        # NNåˆ†æ”¯ (ä¸“é—¨é’ˆå¯¹NNçš„å¤æ‚è®¾è®¡)
        self.nn_branch = nn.Sequential(
            nn.Linear(base_encoded_dim, hidden_space + 32),  # æ›´å¤§å®¹é‡
            nn.LayerNorm(hidden_space + 32),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.1),  # æ›´ä½dropout
            nn.Linear(hidden_space + 32, hidden_space + 16),
            nn.LayerNorm(hidden_space + 16),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.1),
            nn.Linear(hidden_space + 16, hidden_space),
            nn.LayerNorm(hidden_space)
        )
        
        # === [ç­–ç•¥4] è‡ªé€‚åº”èåˆå±‚ ===
        self.adaptive_fusion = nn.Sequential(
            nn.Linear(hidden_space * 2, hidden_space),
            nn.LayerNorm(hidden_space),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.15)
        )
        
        # === [ç­–ç•¥5] NNä¸“ç”¨çš„æ·±åº¦æ³¨æ„åŠ› ===
        self.nn_deep_attention = nn.ModuleList([
            nn.MultiheadAttention(
                embed_dim=hidden_space, 
                num_heads=num_heads//4,
                dropout=dropout_rate * 0.1,
                batch_first=True
            ) for _ in range(3)  # 3å±‚æ·±åº¦æ³¨æ„åŠ›
        ])
        
        # === [ç­–ç•¥6] UDDSä¿æŒçš„ç®€å•æ³¨æ„åŠ› ===
        self.udds_attention = nn.MultiheadAttention(
            embed_dim=hidden_space, 
            num_heads=num_heads//2,
            dropout=dropout_rate * 0.2,
            batch_first=True
        )
        
        # === [ç­–ç•¥7] ä¸»è¦Transformer (é’ˆå¯¹NNä¼˜åŒ–) ===
        from model2 import TimeSeriesTransformer_ekan
        
        self.main_transformer = TimeSeriesTransformer_ekan(
            input_dim=hidden_space,
            num_heads=num_heads,
            num_layers=num_layers,
            num_outputs=num_outputs,
            hidden_space=hidden_space,
            dropout_rate=dropout_rate * 0.5,  # é™ä½dropoutæå‡NNæ€§èƒ½
            embed_dim=embed_dim,
            grid_size=grid_size,
            degree=5,
            use_residual_scaling=True
        )
        
        # === [ç­–ç•¥8] å·¥å†µç‰¹å®šçš„é¢„æµ‹å¤´ ===
        self.nn_prediction_head = nn.Sequential(
            nn.Linear(hidden_space, hidden_space//2),
            nn.LayerNorm(hidden_space//2),
            nn.GELU(),
            nn.Dropout(dropout_rate * 0.1),
            nn.Linear(hidden_space//2, hidden_space//4),
            nn.LayerNorm(hidden_space//4),
            nn.GELU(),
            nn.Linear(hidden_space//4, num_outputs)
        )
        
        self.udds_prediction_head = nn.Sequential(
            nn.Linear(hidden_space, hidden_space//3),
            nn.LayerNorm(hidden_space//3),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.2),
            nn.Linear(hidden_space//3, num_outputs)
        )
        
        # === ç”µåŒ–å­¦ç‰©ç†çº¦æŸå±‚ ===
        self.electrochemical_constraint = TrueSOTAConstraintLayer(num_outputs, temperature, feature_scaler=feature_scaler)
        
        # æ–°å¢ï¼šå¯å­¦ä¹ çš„èåˆæƒé‡å‚æ•°
        self.fusion_weight_param = nn.Parameter(torch.tensor(0.5)) # åˆå§‹å€¼0.5ï¼Œè¡¨ç¤ºå¹³å‡èåˆ
        self.fusion_weight_head_param = nn.Parameter(torch.tensor(0.5)) # Í·ÊµÈ¨
        
    def forward(self, x, return_uncertainty=False):
        batch_size, seq_len, features = x.shape
        
        # === ç‰¹å¾ç¼–ç  ===
        basic_encoded = self.basic_encoder(x[:, :, :10])
        dynamic_encoded = self.dynamic_encoder(x[:, :, 10:22])
        energy_encoded = self.energy_encoder(x[:, :, 22:30])
        impedance_encoded = self.impedance_encoder(x[:, :, 30:40])
        temp_encoded = self.temperature_encoder(x[:, :, 40:46])
        
        # æ‹¼æ¥ç‰¹å¾
        concatenated_features = torch.cat([
            basic_encoded, dynamic_encoded, energy_encoded, 
            impedance_encoded, temp_encoded
        ], dim=-1)
        
        # === å·¥å†µæ£€æµ‹ - ä½¿ç”¨1Dæ³¨æ„åŠ›æ± åŒ– ===
        attention_weights = self.attention_pooling(concatenated_features) # [batch, seq_len, 1]
        detector_input = torch.sum(concatenated_features * attention_weights, dim=1) # [batch, base_encoded_dim]
        detector_logits = self.workload_detector(detector_input)
        temp = torch.clamp(self.detector_temperature, min=0.2, max=5.0)
        workload_probs = F.softmax(detector_logits / temp, dim=-1)  # [batch, 2]
        nn_prob = workload_probs[:, 0]      # NNå·¥å†µæ¦‚ç‡
        udds_prob = workload_probs[:, 1]    # UDDSå·¥å†µæ¦‚ç‡
        # print(f"DEBUG: workload_probs (NN, UDDS): mean_nn={nn_prob.mean().item():.4f}, mean_udds={udds_prob.mean().item():.4f}, std_nn={nn_prob.std().item():.4f}, std_udds={udds_prob.std().item():.4f}")
        
        # === å·¥å†µç‰¹å®šå¤„ç† ===
        udds_features = self.udds_branch(concatenated_features)
        nn_features = self.nn_branch(concatenated_features)
        
        # === å·¥å†µç‰¹å®šæ³¨æ„åŠ› ===
        # UDDS: ç®€å•æ³¨æ„åŠ› (ä¿æŒä¼˜åŠ¿)
        udds_attended, _ = self.udds_attention(udds_features, udds_features, udds_features)
        udds_enhanced = udds_features + udds_attended
        
        # NN: æ·±åº¦æ³¨æ„åŠ› (ä¸“é—¨ä¼˜åŒ–)
        nn_enhanced = nn_features
        for attention_layer in self.nn_deep_attention:
            nn_attended, _ = attention_layer(nn_enhanced, nn_enhanced, nn_enhanced)
            nn_enhanced = nn_enhanced + nn_attended  # æ®‹å·®è¿æ¥
        
        # === è‡ªé€‚åº”èåˆ ===
        # æ ¹æ®å·¥å†µæ¦‚ç‡åŠ¨æ€èåˆ
        nn_weight = nn_prob.unsqueeze(1).unsqueeze(2)
        udds_weight = udds_prob.unsqueeze(1).unsqueeze(2)
        
        weighted_features = nn_enhanced * nn_weight + udds_enhanced * udds_weight
        
        # === ä¸»è¦Transformerå¤„ç† ===
        # Transformerè¾“å‡ºå½¢çŠ¶: [batch, num_outputs] (å› ä¸ºTimeSeriesTransformer_ekanå†…éƒ¨å·²å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥)
        transformer_output = self.main_transformer(weighted_features)
        # print(f"DEBUG: transformer_output (åŸå§‹): min={transformer_output.min().item():.4f}, max={transformer_output.max().item():.4f}, mean={transformer_output.mean().item():.4f}, std={transformer_output.std().item():.4f}")
        # æ­¤æ—¶ transformer_output å½¢çŠ¶ä¸º [batch, num_outputs]ï¼Œæ— éœ€å†å–[:, -1, :]
        
        # === å·¥å†µç‰¹å®šé¢„æµ‹ ===
        # é¢„æµ‹å¤´å·²ç»è¾“å‡º [batch, num_outputs] (ç°åœ¨æ˜¯logits)
        nn_pred_logits = self.nn_prediction_head(weighted_features.mean(dim=1))
        udds_pred_logits = self.udds_prediction_head(weighted_features.mean(dim=1))
        # print(f"DEBUG: nn_pred_logits: min={nn_pred_logits.min().item():.4f}, max={nn_pred_logits.max().item():.4f}, mean={nn_pred_logits.mean().item():.4f}, std={nn_pred_logits.std().item():.4f}")
        # print(f"DEBUG: udds_pred_logits: min={udds_pred_logits.min().item():.4f}, max={udds_pred_logits.max().item():.4f}, mean={udds_pred_logits.mean().item():.4f}, std={udds_pred_logits.std().item():.4f}")

        # åŠ æƒé¢„æµ‹ (ç›´æ¥ä½¿ç”¨ logits èåˆ)
        # nn_weight å’Œ udds_weight å½¢çŠ¶ä¸º [batch, 1, 1]ï¼Œéœ€è¦è°ƒæ•´ä¸º [batch, 1]
        final_prediction_weighted_logits = (nn_pred_logits * nn_weight.squeeze(1) + 
                                           udds_pred_logits * udds_weight.squeeze(1))
        # print(f"DEBUG: final_prediction_weighted_logits: min={final_prediction_weighted_logits.min().item():.4f}, max={final_prediction_weighted_logits.max().item():.4f}, mean={final_prediction_weighted_logits.mean().item():.4f}, std={final_prediction_weighted_logits.std().item():.4f}")

        # åŠ¨æ€èåˆtransformerè¾“å‡ºå’Œç‰¹å®šé¢„æµ‹
        raw_w_t = torch.sigmoid(self.fusion_weight_param)
        raw_w_h = torch.sigmoid(self.fusion_weight_head_param)
        norm = raw_w_t + raw_w_h + 1e-6
        fusion_weight_transformer = raw_w_t / norm
        fusion_weight_head = raw_w_h / norm
        
        combined_output_last_step_logits = fusion_weight_transformer * transformer_output + fusion_weight_head * final_prediction_weighted_logits
        # print(f"DEBUG: combined_output_last_step (èåˆå logits): min={combined_output_last_step_logits.min().item():.4f}, max={combined_output_last_step_logits.max().item():.4f}, mean={combined_output_last_step_logits.mean().item():.4f}, std={combined_output_last_step_logits.std().item():.4f}")
        
        # === ç‰©ç†çº¦æŸ ===
        # å°† logits ä¼ å…¥ç‰©ç†çº¦æŸå±‚
        constrained_output = self.electrochemical_constraint(combined_output_last_step_logits.unsqueeze(1), x[:, -1, :].unsqueeze(1)) # ç‰©ç†çº¦æŸå±‚å°†å¤„ç† Sigmoid æ¿€æ´»å’Œ Clamp
        # print(f"DEBUG: constrained_output (ç‰©ç†çº¦æŸå±‚è¾“å‡º): min={constrained_output.min().item():.4f}, max={constrained_output.max().item():.4f}, mean={constrained_output.mean().item():.4f}, std={constrained_output.std().item():.4f}")
        # å› ä¸ºçº¦æŸå±‚å†…éƒ¨ç°åœ¨æœŸæœ› [batch, seq_len, num_outputs] å¹¶ä¸”å·²ç»å°†å…¶å±•å¹³å¤„ç†ï¼Œ
        # æˆ‘ä»¬è¿™é‡Œè¾“å…¥çš„æ˜¯ [batch, 1, num_outputs] å¹¶ä¸” x[:, -1, :].unsqueeze(1) ä¹Ÿæ˜¯ [batch, 1, num_outputs]
        # çº¦æŸå±‚çš„è¾“å‡ºä¼šæ˜¯ [batch, 1, num_outputs]ï¼Œæˆ‘ä»¬éœ€è¦å…¶æœ€ç»ˆæ˜¯ [batch, num_outputs]
        constrained_output = constrained_output.squeeze(1)
        
        if return_uncertainty:
            return constrained_output, workload_probs
        
        return constrained_output


class TrueSOTAElectrochemicalPhysicsLoss(nn.Module):
    """çœŸæ­£SOTAçš„ç”µåŒ–å­¦ç‰©ç†æŸå¤±å‡½æ•°"""
    
    def __init__(self, base_weight=1.0, electrochemical_weight=1.5, resistance_loss_factor=0.001, feature_scaler=None): # æ–°å¢ feature_scaler å’Œ resistance_loss_factor
        super().__init__()
        self.base_weight = base_weight
        self.electrochemical_weight = nn.Parameter(torch.tensor(electrochemical_weight)) # åˆå§‹å€¼ä»0.05æ”¹ä¸º1.5
        self.loss_balancer = nn.Parameter(torch.tensor(2.0)) # æé«˜loss_balancerçš„åˆå§‹å€¼åˆ°2.0
        
        # å·¥å†µç‰¹å®šæŸå¤±æƒé‡
        self.nn_loss_weight = nn.Parameter(torch.tensor(1.5))    # æé«˜ nn_loss_weight
        self.udds_loss_weight = nn.Parameter(torch.tensor(0.8))  # é™ä½ udds_loss_weight
        self.feature_scaler = feature_scaler # å­˜å‚¨æ ‡å‡†åŒ–å™¨
        self.resistance_loss_factor = resistance_loss_factor # å­˜å‚¨å†…é˜»æŸå¤±å› å­
        
    def forward(self, predictions, targets, inputs, workload_probs=None):
        # åŸºç¡€MSEæŸå¤±
        mse_loss = F.mse_loss(predictions, targets) # predictions å’Œ targets éƒ½åº”è¯¥åœ¨ 0-1 èŒƒå›´
        
        # ç”µåŒ–å­¦ç‰©ç†æŸå¤±
        electrochemical_loss = self._compute_electrochemical_loss(predictions, targets, inputs)
        
        # å·¥å†µè‡ªé€‚åº”æŸå¤±åŠ æƒ
        if workload_probs is not None:
            nn_prob = workload_probs[:, 0].mean()
            udds_prob = workload_probs[:, 1].mean()
            
            # ç¡®ä¿æŸå¤±æƒé‡å§‹ç»ˆä¸ºæ­£
            nn_weight = F.softplus(self.nn_loss_weight)
            udds_weight = F.softplus(self.udds_loss_weight)
            
            adaptive_weight = nn_weight * nn_prob + udds_weight * udds_prob
        else:
            adaptive_weight = 1.0
        
        # æŸå¤±å¹³è¡¡
        electrochemical_weight = self.electrochemical_weight # ç›´æ¥ä½¿ç”¨ Parameter çš„åŸå§‹å€¼
        loss_balance = torch.sigmoid(self.loss_balancer)
        
        total_loss = (
            self.base_weight * mse_loss * loss_balance * adaptive_weight + 
            electrochemical_weight * electrochemical_loss * (1 - loss_balance * 0.5)
        )
        
        return total_loss, {
            'mse_loss': mse_loss.item(), 
            'electrochemical_loss': electrochemical_loss.item(),
            'adaptive_weight': adaptive_weight.item() if isinstance(adaptive_weight, torch.Tensor) else adaptive_weight
        }
    
    def _compute_electrochemical_loss(self, predictions, targets, inputs):
        """è®¡ç®—ç”µåŒ–å­¦ç‰©ç†æŸå¤±"""
        loss_components = []

        # ç¡®ä¿ predictions å’Œ targets å·²ç»æ˜¯ç‰©ç†å€¼ (0-1ä¹‹é—´)
        # ç§»é™¤æ‰€æœ‰ target_scaler.inverse_transform çš„è°ƒç”¨
        predictions_physical = predictions
        targets_physical = targets

        # ä»¥ä¸‹æ‰€æœ‰æŸå¤±è®¡ç®—éƒ½åŸºäº predictions_physical å’Œ targets_physical

        # å¤„ç†ä¸åŒç»´åº¦çš„predictions (ç°åœ¨éƒ½æ˜¯ç‰©ç†å°ºåº¦)
        # ç”±äºç°åœ¨ predictions_physical æ˜¯ [batch, num_outputs] å½¢çŠ¶ï¼Œä¸éœ€è¦å†è¿›è¡Œ `if len(predictions_physical.shape) == 3` åˆ¤æ–­
        # ç›´æ¥è¿›å…¥ [batch, features] çš„å¤„ç†é€»è¾‘
        # SOCèŒƒå›´çº¦æŸ
        soc_pred_original = predictions_physical[:, 0]  # ç°åœ¨å·²ç»æ˜¯ç‰©ç†SOC
        range_loss = F.relu(soc_pred_original - 1.0).mean() + F.relu(-soc_pred_original).mean()
        loss_components.append(range_loss) # ç¡®ä¿ range_loss èƒ½å¤Ÿæä¾›è¶³å¤Ÿæ¢¯åº¦
        
        # SOC-SOEä¸€è‡´æ€§çº¦æŸ
        if predictions_physical.shape[1] >= 2:
            soe_pred_original = predictions_physical[:, 1] # ç°åœ¨å·²ç»æ˜¯ç‰©ç†SOE
            consistency_loss = F.relu(torch.abs(soe_pred_original - soc_pred_original) - 0.12).mean() # 0.12 å·®è·å®¹å¿åº¦
            loss_components.append(consistency_loss)
        
        # ç”µåŒ–å­¦å¹³æ»‘æ€§çº¦æŸ (ä¸é€‚ç”¨å•æ—¶é—´æ­¥é¢„æµ‹ï¼Œä¿æŒæ³¨é‡Š)
        # if predictions_physical.shape[0] > 1:  # æ£€æŸ¥batchç»´åº¦
        #     try:
        #         original_predictions_smooth = predictions_physical.unsqueeze(1).clone() # å¢åŠ seq_lenç»´åº¦
        #         current_targets_smooth = targets_physical.unsqueeze(1).clone() # å¢åŠ seq_lenç»´åº¦

        #         pred_diff = torch.diff(original_predictions_smooth, dim=1) 
        #         target_diff = torch.diff(current_targets_smooth, dim=1)    

        #         if pred_diff.shape == target_diff.shape: # type: ignore
        #             smoothness_loss = F.mse_loss(pred_diff, target_diff) * 0.06
        #             loss_components.append(smoothness_loss)
        #     except RuntimeError as e:
        #         print(f"âš ï¸ å¹³æ»‘æ€§çº¦æŸè®¡ç®—è­¦å‘Š: {e}") 
        #         pass
        
        # åŸºäºå†…é˜»çš„ç¨³å®šæ€§çº¦æŸ (é‡æ–°å¼•å…¥)
        if inputs.shape[-1] > 6 and self.feature_scaler is not None: # ç¡®ä¿æœ‰å†…é˜»ç‰¹å¾å’Œscaler
            # è·å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„å†…é˜»ç‰¹å¾ï¼ˆæ ‡å‡†åŒ–åçš„ï¼‰
            normalized_resistance_feature_last_step = inputs[:, -1, 5] # ç´¢å¼•5é€šå¸¸æ˜¯å†…é˜»
            
            # å¯¹å†…é˜»è¿›è¡Œé€†æ ‡å‡†åŒ–ï¼Œå¾—åˆ°åŸå§‹ç‰©ç†å€¼
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯feature_scalerï¼Œå®ƒç°åœ¨æ˜¯StandardScaler
            # StandardScalerçš„é€†å˜æ¢ï¼šoriginal_value = normalized_value * std + mean
            mean_resistance = torch.tensor(self.feature_scaler.mean_[5], device=inputs.device, dtype=inputs.dtype)
            std_resistance = torch.tensor(self.feature_scaler.scale_[5], device=inputs.device, dtype=inputs.dtype)
            
            original_resistance_feature = normalized_resistance_feature_last_step * std_resistance + mean_resistance
            
            # å†…é˜»åŠ æƒé¢„æµ‹æ³¢åŠ¨æƒ©ç½š (é¼“åŠ±é«˜å†…é˜»æ—¶é¢„æµ‹æ›´å¹³ç¨³)
            # è¿™é‡Œæˆ‘ä»¬é¼“åŠ±é¢„æµ‹å€¼ä¸è¦è¿‡å¤§ï¼Œåœ¨é«˜å†…é˜»æ—¶å°¤å…¶å¦‚æ­¤
            # predictions_physical å½¢çŠ¶ä¸º [batch, num_outputs]ï¼Œæ‰€ä»¥å¯¹æœ€åä¸€ä¸ªç»´åº¦æ±‚å’Œ
            resistance_penalty = torch.mean(original_resistance_feature * torch.sum(predictions_physical**2, dim=-1)) * self.resistance_loss_factor
            loss_components.append(resistance_penalty)
        
        return sum(loss_components) if loss_components else torch.tensor(0.0, device=predictions.device)


class TrueSOTAElectrochemicalLightningModule(pl.LightningModule):
    """çœŸæ­£SOTAçš„ç”µåŒ–å­¦Lightningæ¨¡å—"""
    
    def __init__(self, hparams, feature_scaler=None): # ç§»é™¤ target_scaler å‚æ•°
        super().__init__()
        if isinstance(hparams, dict): 
            hparams = argparse.Namespace(**hparams)
        self.save_hyperparameters(hparams)
        
        self.model = TrueSOTAElectrochemicalKANTransformer(
            input_dim=46,
            num_heads=hparams.num_heads, 
            num_layers=hparams.n_layers,
            num_outputs=len(hparams.output_features), 
            hidden_space=hparams.hidden_space,
            dropout_rate=hparams.dropout, 
            embed_dim=hparams.embed_dim, 
            grid_size=hparams.grid_size,
            temperature=getattr(hparams, 'temperature', None),
            feature_scaler=feature_scaler, # ä¼ é€’ç‰¹å¾æ ‡å‡†åŒ–å™¨ç»™æ¨¡å‹
            detector_temp_init=getattr(hparams, 'detector_temp_init', 0.7)
        )
        
        self.criterion = TrueSOTAElectrochemicalPhysicsLoss(
            electrochemical_weight=getattr(hparams, 'electrochemical_weight', 0.5),
            resistance_loss_factor=getattr(hparams, 'resistance_loss_factor', 0.001), # æ–°å¢ï¼šä¼ é€’ resistance_loss_factor
            feature_scaler=feature_scaler # ä¼ é€’ç‰¹å¾æ ‡å‡†åŒ–å™¨ç»™æŸå¤±å‡½æ•°
        )
        self.gate_entropy_weight = getattr(hparams, "gate_entropy_weight", 0.01)
        
        self.automatic_optimization = True
        self.test_step_outputs = []
        self.current_epoch_num = 0
        # self.feature_scaler = feature_scaler # å­˜å‚¨æ ‡å‡†åŒ–å™¨ - ç§»è‡³æ¨¡å‹æ„é€ å‡½æ•°
        
    def forward(self, x): 
        return self.model(x)
    
    def training_step(self, batch, batch_idx):
        x, y, _ = batch # æ•°æ®æ¨¡å—ç°åœ¨åªè¿”å›3ä¸ªå…ƒç´ ï¼šx, y, original_end_indices
        
        # æ•°æ®å¢å¼º (é’ˆå¯¹NNä¼˜åŒ–)
        if self.training:
            # ä¿å®ˆçš„å™ªå£°æ³¨å…¥
            noise_factor = self.hparams.noise_factor * (1 - self.current_epoch_num / self.hparams.num_epochs)
            if noise_factor > 0: 
                x += torch.randn_like(x) * noise_factor
        
        # å‰å‘ä¼ æ’­
        y_hat, workload_probs = self.model(x, return_uncertainty=True)
        total_loss, loss_components = self.criterion(y_hat, y, x, workload_probs)
        gate_entropy = -(workload_probs * torch.log(torch.clamp(workload_probs, min=1e-6))).sum(dim=1).mean()
        total_loss = total_loss + self.gate_entropy_weight * gate_entropy
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss): 
            total_loss = F.mse_loss(y_hat, y)
        
        # æ—¥å¿—è®°å½•
        self.log('train_loss', total_loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log_dict({f'train_{k}': v for k, v in loss_components.items()}, on_step=False, on_epoch=True)
        self.log('gate_entropy', gate_entropy, on_step=False, on_epoch=True)
        
        with torch.no_grad(): 
            train_rmse = torch.sqrt(F.mse_loss(y_hat, y))
            self.log('train_rmse', train_rmse, on_step=False, on_epoch=True)
            
        return total_loss
    
    def validation_step(self, batch, batch_idx):
        x, y, _ = batch # æ•°æ®æ¨¡å—ç°åœ¨åªè¿”å›3ä¸ªå…ƒç´ ï¼šx, y, original_end_indices
        y_hat = self.forward(x)
        val_loss = F.mse_loss(y_hat, y)
        val_rmse = torch.sqrt(val_loss)
        
        self.log('val_loss', val_loss, on_epoch=True, prog_bar=True)
        self.log('val_rmse', val_rmse, on_epoch=True, prog_bar=True)
    
    def test_step(self, batch, batch_idx, dataloader_idx=0):
        x, y, original_end_indices = batch # æ¥æ”¶åŸå§‹ç»“æŸç´¢å¼•
        y_hat = self.forward(x)
        self.test_step_outputs[dataloader_idx].append({'y_true': y.cpu(), 'y_pred': y_hat.cpu(), 'original_end_indices': original_end_indices.cpu()})
    
    def on_test_start(self): 
        self.test_step_outputs = [[] for _ in range(2)]
    
    def on_test_epoch_end(self):
        print("\n" + "="*80)
        print("ğŸ¯ 25Â°CçœŸæ­£SOTAæ¨¡å‹æµ‹è¯•ç»“æœ (SCIè®ºæ–‡çº§åˆ«å›¾è¡¨)")
        print("="*80)
        
        # è®¾ç½®SCIè®ºæ–‡çº§åˆ«æ ·å¼
        setup_sci_style()
        
        # åˆ›å»ºSCIçº§åˆ«å›¾è¡¨ - å®Œå…¨å‚ç…§ç”¨æˆ·æä¾›çš„å‚è€ƒå›¾
        fig, axes = plt.subplots(2, 4, figsize=(16, 8))
        fig.suptitle('Fig. 10: Actual and estimate values of SOC and SOE, and the estimation error with different drive cycles under 25Â°C: LA92 (top); UDDS (bottom).', 
                    fontsize=12, fontweight='normal', y=0.02)  # ç§»åˆ°å›¾ç‰‡ä¸‹æ–¹
        
        dataset_names = ["LA92", "UDDS"]
        subplot_idx = 0
        overall_results = {}
        
        for i, outputs in enumerate(self.test_step_outputs):
            if not outputs: continue
            
            # å¤„ç†ä¸åŒåºåˆ—é•¿åº¦çš„å¼ é‡è¿æ¥é—®é¢˜
            y_true_list = []
            y_pred_list = []
            original_end_indices_list = [] # æ–°å¢ï¼šç”¨äºæ”¶é›†åŸå§‹ç´¢å¼•
            for x in outputs:
                # ç»Ÿä¸€å¤„ç†ï¼šç¡®ä¿éƒ½æ˜¯2D [batch, features]
                if len(x['y_true'].shape) == 3:  # [batch, seq_len, features]
                    y_true_list.append(x['y_true'][:, -1, :])  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                    original_end_indices_list.append(x['original_end_indices'][:, -1]) # æ”¶é›†åŸå§‹ç´¢å¼•
                else:  # [batch, features]
                    y_true_list.append(x['y_true'])
                    original_end_indices_list.append(x['original_end_indices']) # æ”¶é›†åŸå§‹ç´¢å¼•
                    
                if len(x['y_pred'].shape) == 3:  # [batch, seq_len, features]
                    y_pred_list.append(x['y_pred'][:, -1, :])  # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥
                else:  # [batch, features]
                    y_pred_list.append(x['y_pred'])
            
            # å®‰å…¨çš„å¼ é‡è¿æ¥ï¼šå¤„ç†ä¸åŒbatchå¤§å°
            try:
                y_true = torch.cat(y_true_list).numpy()
                y_pred = torch.cat(y_pred_list).numpy()
                original_end_indices = torch.cat(original_end_indices_list).numpy() # åˆå¹¶åŸå§‹ç´¢å¼•
            except RuntimeError as e:
                print(f"âš ï¸  å¼ é‡è¿æ¥è­¦å‘Š: {str(e)}")
                # å¦‚æœä»ç„¶å¤±è´¥ï¼Œé€ä¸ªæ£€æŸ¥å¹¶ä¿®å¤ç»´åº¦
                fixed_y_true_list = []
                fixed_y_pred_list = []
                fixed_original_end_indices_list = [] # æ–°å¢ï¼šç”¨äºæ”¶é›†ä¿®å¤åçš„åŸå§‹ç´¢å¼•
                for yt, yp, oei in zip(y_true_list, y_pred_list, original_end_indices_list):
                    # ç¡®ä¿éƒ½æ˜¯2Dä¸”ç‰¹å¾æ•°ä¸€è‡´
                    if len(yt.shape) == 2 and len(yp.shape) == 2:
                        if yt.shape[1] == yp.shape[1]:  # ç‰¹å¾æ•°ä¸€è‡´
                            fixed_y_true_list.append(yt)
                            fixed_y_pred_list.append(yp)
                            fixed_original_end_indices_list.append(oei) # æ”¶é›†ä¿®å¤åçš„åŸå§‹ç´¢å¼•
                
                y_true = torch.cat(fixed_y_true_list).numpy()
                y_pred = torch.cat(fixed_y_pred_list).numpy()
                original_end_indices = torch.cat(fixed_original_end_indices_list).numpy() # åˆå¹¶ä¿®å¤åçš„åŸå§‹ç´¢å¼•
            
            dataset_name = dataset_names[i]
            
            # --- æ–°å¢: ä¿å­˜åŸå§‹é¢„æµ‹æ•°æ® ---
            save_dir = os.path.join(self.trainer.logger.log_dir or '.', 'raw_predictions')
            os.makedirs(save_dir, exist_ok=True)
            
            model_identifier = "KAN-Transformer_46feat"
            true_path = os.path.join(save_dir, f'{model_identifier}_{dataset_name}_true.npy')
            pred_path = os.path.join(save_dir, f'{model_identifier}_{dataset_name}_pred.npy')
            
            np.save(true_path, y_true)
            np.save(pred_path, y_pred)
            # åŒæ—¶ä¿å­˜åŸå§‹æ—¶é—´è½´ç´¢å¼•ï¼Œä»¥ä¾¿ç”Ÿæˆ_true.npyå’Œ_pred.npyæ—¶ä½œä¸ºæ¨ªåæ ‡
            np.save(os.path.join(save_dir, f'{model_identifier}_{dataset_name}_time_axis.npy'), original_end_indices)
            print(f"   ğŸ“Š {model_identifier} {dataset_name} åŸå§‹é¢„æµ‹æ•°æ®å·²ä¿å­˜: {pred_path}")
            print(f"   ğŸ“Š {model_identifier} {dataset_name} åŸå§‹æ—¶é—´è½´ç´¢å¼•å·²ä¿å­˜: {os.path.join(save_dir, f'{model_identifier}_{dataset_name}_time_axis.npy')}")
            # --- æ–°å¢ç»“æŸ ---

            time_axis = original_end_indices # ä½¿ç”¨åŸå§‹æ—¶é—´è½´ç´¢å¼•ä½œä¸ºæ¨ªåæ ‡
            
            for j, feature in enumerate(self.hparams.output_features):
                # === SCIçº§åˆ«é¢„æµ‹ç»“æœå›¾ - å®Œå…¨å‚ç…§ç”¨æˆ·å‚è€ƒå›¾ ===
                ax_pred = axes[i, j*2]
                
                actual_values = y_true[:, j] * 100
                pred_values = y_pred[:, j] * 100
                
                # è½»å¾®å¹³æ»‘å¤„ç† (å‚è€ƒå›¾çš„å¹³æ»‘åº¦)
                actual_smooth = smooth_data(actual_values, window_length=9, polyorder=2)
                pred_smooth = smooth_data(pred_values, window_length=9, polyorder=2)
                
                # å‚è€ƒå›¾çš„ç²¾ç¡®é¢œè‰²æ–¹æ¡ˆ - éƒ½æ˜¯å®çº¿
                ax_pred.plot(time_axis, actual_smooth, color='#0072BD', linewidth=1.8, 
                           label='Actual Value', alpha=1.0)      # è“è‰²å®çº¿
                ax_pred.plot(time_axis, pred_smooth, color='#D95319', linewidth=1.8, 
                           label='Estimated Value', alpha=1.0)   # æ©™çº¢è‰²å®çº¿
                
                ax_pred.set_xlabel('Time(s)', fontsize=11)
                ax_pred.set_ylabel(f'{feature}(%)', fontsize=11)
                ax_pred.set_title(f'({chr(97+subplot_idx)})', fontsize=12, fontweight='normal')
                ax_pred.legend(loc='upper right', frameon=False, fontsize=10)
                ax_pred.grid(True, alpha=0.2, linestyle='-', linewidth=0.5)  # å‚è€ƒå›¾çš„ç»†ç½‘æ ¼
                ax_pred.set_ylim(0, 100)
                
                # å‚è€ƒå›¾çš„è¾¹æ¡†æ ·å¼
                for spine in ax_pred.spines.values():
                    spine.set_linewidth(1.0)
                    spine.set_color('black')
                
                # === SCIçº§åˆ«è¯¯å·®åˆ†æå›¾ - å‚ç…§å‚è€ƒå›¾ ===
                ax_error = axes[i, j*2 + 1]
                
                error_values = pred_smooth - actual_smooth
                error_smooth = smooth_data(error_values, window_length=7, polyorder=2)
                
                # å‚è€ƒå›¾çš„ç²¾ç¡®è¯¯å·®é¢œè‰² - æ·±è“è‰²ç³» (ä¸å‚è€ƒå›¾å®Œå…¨ä¸€è‡´)
                ax_error.plot(time_axis, error_smooth, color='#1f77b4', linewidth=1.8, alpha=1.0)
                ax_error.set_xlabel('Time(s)', fontsize=11)
                ax_error.set_ylabel(f'{feature} Error (%)', fontsize=11)
                ax_error.set_title(f'({chr(97+subplot_idx+1)})', fontsize=12, fontweight='normal')
                ax_error.grid(True, alpha=0.2, linestyle='-', linewidth=0.5)  # å‚è€ƒå›¾çš„ç»†ç½‘æ ¼
                ax_error.axhline(y=0, color='black', linestyle='-', alpha=0.4, linewidth=0.8)
                ax_error.set_ylim(-6, 6)  # ç»Ÿä¸€errorå›¾çºµåæ ‡èŒƒå›´ä¸º-6åˆ°6
                
                # å‚è€ƒå›¾çš„è¾¹æ¡†æ ·å¼
                for spine in ax_error.spines.values():
                    spine.set_linewidth(1.0)
                    spine.set_color('black')
                
                # è¯„ä¼°æŒ‡æ ‡è®¡ç®—
                rmse = np.sqrt(mean_squared_error(y_true[:, j], y_pred[:, j]))
                mae = mean_absolute_error(y_true[:, j], y_pred[:, j])
                r2 = r2_score(y_true[:, j], y_pred[:, j])
                
                result_key = f"{dataset_name}_{feature}"
                overall_results[result_key] = {'RMSE': rmse, 'MAE': mae, 'R2': r2}
                
                # ä¸åŸºå‡†å¯¹æ¯”
                original_baseline = {"LA92_SOC": 0.030915, "LA92_SOE": 0.029856, "UDDS_SOC": 0.012418, "UDDS_SOE": 0.011143}
                tabpfn_baseline = {"LA92_SOC": 0.029, "LA92_SOE": 0.027, "UDDS_SOC": 0.012, "UDDS_SOE": 0.010}
                
                original_rmse = original_baseline.get(result_key, 0.025)
                tabpfn_rmse = tabpfn_baseline.get(result_key, 0.025)
                
                improvement_vs_original = original_rmse - rmse
                improvement_vs_tabpfn = tabpfn_rmse - rmse
                
                print(f"ğŸ¯ {dataset_name} - {feature}:")
                print(f"    RMSE={rmse:.6f}, MAE={mae:.6f}, RÂ²={r2:.6f}")
                
                # ç‰¹åˆ«æ ‡æ³¨æ€§èƒ½å˜åŒ–
                if dataset_name == "LA92":
                    if rmse < 0.020:
                        print(f"    ğŸš€ LA92è¡¨ç°ä¼˜ç§€! RMSE: {rmse:.6f} < 0.020")
                    else:
                        print(f"    ğŸ“ˆ LA92è¡¨ç°: RMSE: {rmse:.6f}")
                elif dataset_name == "UDDS":
                    if rmse <= 0.013:
                        print(f"    âœ… UDDSä¿æŒä¼˜åŠ¿: {rmse:.6f} â‰¤ 0.013")
                    else:
                        print(f"    âš ï¸ UDDSç•¥æœ‰é€€æ­¥: {rmse:.6f} > 0.013")
                
                if improvement_vs_tabpfn > 0:
                    print(f"    ğŸ† è¶…è¶ŠTabPFN! æ”¹å–„: {improvement_vs_tabpfn:.6f}")
                else:
                    print(f"    ğŸ“Š vs TabPFNå·®è·: {abs(improvement_vs_tabpfn):.6f}")
                
                subplot_idx += 2
        
        # å‚è€ƒå›¾çš„ç²¾ç¡®å¸ƒå±€è°ƒæ•´ - ä¸ºåº•éƒ¨æ ‡é¢˜ç•™å‡ºç©ºé—´
        plt.tight_layout()
        plt.subplots_adjust(top=0.95, bottom=0.15, left=0.06, right=0.98, 
                           hspace=0.35, wspace=0.25)  # å¢åŠ åº•éƒ¨ç©ºé—´ç»™Fig.10è¯´æ˜
        
        # ä¿å­˜é«˜è´¨é‡SCIå›¾è¡¨ - å‚ç…§å‚è€ƒå›¾æ ¼å¼
        save_path = os.path.join(self.trainer.logger.log_dir or '.', 
                               'true_sota_25degC_SCI_paper_style.png')
        plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white', 
                    edgecolor='none', format='png', pad_inches=0.1)
        print(f"\nğŸ“Š SCIè®ºæ–‡çº§åˆ«å›¾è¡¨å·²ä¿å­˜: {save_path}")
        
        # ä¿å­˜æµ‹è¯•æŒ‡æ ‡
        results_df = pd.DataFrame(overall_results).T
        results_df.index.name = 'Dataset_Feature'
        results_csv_path = os.path.join(self.trainer.logger.log_dir or '.', 
                                      'true_sota_25degC_test_metrics.csv')
        results_df.to_csv(results_csv_path)
        print(f"ğŸ“Š æµ‹è¯•æŒ‡æ ‡ç»“æœå·²ä¿å­˜: {results_csv_path}")
        
        # ç»¼åˆæ€§èƒ½åˆ†æ
        avg_rmse = np.mean([r['RMSE'] for r in overall_results.values()])
        avg_mae = np.mean([r['MAE'] for r in overall_results.values()])
        avg_r2 = np.mean([r['R2'] for r in overall_results.values()])
        
        # åˆ†å·¥å†µåˆ†æ
        la92_rmse = np.mean([overall_results[k]['RMSE'] for k in overall_results.keys() if 'LA92' in k])
        udds_rmse = np.mean([overall_results[k]['RMSE'] for k in overall_results.keys() if 'UDDS' in k])
        
        print(f"\nğŸ† ç»¼åˆæ€§èƒ½ (SCIè®ºæ–‡çº§åˆ«):")
        print(f"    å¹³å‡RMSE: {avg_rmse:.6f}")
        print(f"    å¹³å‡MAE:  {avg_mae:.6f}")
        print(f"    å¹³å‡RÂ²:   {avg_r2:.6f}")
        
        print(f"\nğŸ“Š åˆ†å·¥å†µæ€§èƒ½:")
        print(f"    LA92å¹³å‡RMSE: {la92_rmse:.6f} (LA92ä½œä¸ºæµ‹è¯•é›†)")
        print(f"    UDDSå¹³å‡RMSE: {udds_rmse:.6f} (ç›®æ ‡: â‰¤0.013)")
        
        # ä¸åŸºå‡†å¯¹æ¯”
        original_avg_rmse = 0.021083
        tabpfn_avg_rmse = 0.0195
        
        print(f"\nğŸ“ˆ SCIè®ºæ–‡çº§åˆ«åˆ†æ:")
        if avg_rmse < original_avg_rmse:
            print(f"    ğŸ‰ æ•´ä½“æ€§èƒ½æå‡: {original_avg_rmse - avg_rmse:.6f}")
        else:
            print(f"    ğŸ“‰ æ•´ä½“æ€§èƒ½é€€æ­¥: {avg_rmse - original_avg_rmse:.6f}")
            
        if avg_rmse < tabpfn_avg_rmse:
            print(f"    ğŸ† æˆåŠŸè¶…è¶ŠTabPFN! æ”¹å–„: {tabpfn_avg_rmse - avg_rmse:.6f}")
        else:
            print(f"    ğŸ“Š è·ç¦»TabPFNè¿˜å·®: {avg_rmse - tabpfn_avg_rmse:.6f}")
        
        # æˆåŠŸæ ‡å‡†
        la92_success = la92_rmse < 0.025  # LA92ç›¸å¯¹å®½æ¾çš„æ ‡å‡†
        udds_success = udds_rmse <= 0.013
        overall_success = avg_rmse < 0.020  # è°ƒæ•´æ•´ä½“æ ‡å‡†
        
        print(f"\nğŸ¯ æˆåŠŸè¯„ä¼° (SCIè®ºæ–‡çº§åˆ«):")
        print(f"    LA92è¡¨ç°: {'âœ…' if la92_success else 'âŒ'} ({la92_rmse:.6f} {'<' if la92_success else 'â‰¥'} 0.025)")
        print(f"    UDDSä¿æŒæˆåŠŸ: {'âœ…' if udds_success else 'âŒ'} ({udds_rmse:.6f} {'â‰¤' if udds_success else '>'} 0.013)")
        print(f"    æ•´ä½“è¡¨ç°: {'âœ…' if overall_success else 'âŒ'} ({avg_rmse:.6f} {'<' if overall_success else 'â‰¥'} 0.020)")
        
        if la92_success and udds_success and overall_success:
            print(f"    ğŸš€ğŸš€ğŸš€ å®Œç¾æˆåŠŸ! SCIè®ºæ–‡çº§åˆ«è¡¨ç°ä¼˜å¼‚!")
        elif la92_success and udds_success:
            print(f"    ğŸš€ğŸš€ åŒé‡æˆåŠŸ! LA92+UDDSéƒ½è¡¨ç°è‰¯å¥½!")
        elif la92_success or udds_success:
            print(f"    ğŸš€ éƒ¨åˆ†æˆåŠŸ! ç»§ç»­ä¼˜åŒ–!")
        
        plt.show()
        print("="*80)
    
    def on_train_epoch_end(self): 
        self.current_epoch_num += 1
    
    def configure_optimizers(self):
        optimizer = torch.optim.AdamW(
            self.parameters(), 
            lr=self.hparams.lr, 
            weight_decay=self.hparams.weight_decay,
            betas=(0.9, 0.95),    # å¹³è¡¡çš„betaå‚æ•°
            eps=1e-6
        )
        
        # ç¨³å®šçš„å­¦ä¹ ç‡è°ƒåº¦
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, 
            mode='min', 
            factor=0.5, 
            patience=15,
            min_lr=self.hparams.lr * 0.001
        )
        
        return {
            "optimizer": optimizer, 
            "lr_scheduler": {"scheduler": scheduler, "monitor": "val_loss", "interval": "epoch"}
        }


class MemoryCleanupCallback(Callback):
    """å†…å­˜æ¸…ç†å›è°ƒ"""
    def on_train_epoch_end(self, trainer, pl_module): 
        gc.collect()
        torch.cuda.empty_cache()


def main():
    # æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
    if torch.cuda.is_available():
        print(f"âœ… PyTorchæ£€æµ‹åˆ°CUDAå¯ç”¨ã€‚è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"   - GPU {i}: {torch.cuda.get_device_name(i)}")
    else:
        print("âŒ PyTorchæœªæ£€æµ‹åˆ°CUDAã€‚è¯·æ£€æŸ¥CUDAå®‰è£…å’Œé©±åŠ¨ã€‚")

    parser = argparse.ArgumentParser(description='25Â°CçœŸæ­£SOTAè®­ç»ƒ - ä¿æŒUDDSä¼˜åŠ¿+æå‡NN')
    
    # === æ•°æ®è·¯å¾„ ===
    # ğŸ”„ è°ƒæ¢é…ç½®ï¼šå°†NNæµ‹è¯•é›†åŠ å…¥è®­ç»ƒï¼ŒLA92è®­ç»ƒé›†ä½œä¸ºæµ‹è¯•
    parser.add_argument('--train_paths', type=str, nargs='+', default=[
        r"C:\25degC training\03-18-17_02.17 25degC_Cycle_1_Pan18650PF.csv",
        r"C:\25degC training\03-19-17_03.25 25degC_Cycle_2_Pan18650PF.csv",
        r"C:\25degC training\03-19-17_09.07 25degC_Cycle_3_Pan18650PF.csv",
        r"C:\25degC training\03-19-17_14.31 25degC_Cycle_4_Pan18650PF.csv",
        r"C:\25degC training\03-20-17_01.43 25degC_US06_Pan18650PF.csv",
        r"C:\25degC training\03-20-17_05.56 25degC_HWFTa_Pan18650PF.csv",
        r"C:\25degC testing\03-21-17_16.27 25degC_NN_Pan18650PF.csv"  # åŸNNæµ‹è¯•é›†ç°åœ¨ç”¨äºè®­ç»ƒ
    ])
    parser.add_argument('--test_paths', type=str, nargs='+', default=[
        r"C:\25degC training\03-21-17_09.38 25degC_LA92_Pan18650PF.csv",  # åŸLA92è®­ç»ƒé›†ç°åœ¨ç”¨äºæµ‹è¯•
        r"C:\25degC testing\03-21-17_00.29 25degC_UDDS_Pan18650PF.csv"   # UDDSä¿æŒä¸å˜
    ])
    parser.add_argument('--result_dir', type=str, default='true_sota_25degC_LA92_test_results_step16')
    parser.add_argument('--output_features', type=str, default='SOC,SOE')
    parser.add_argument('--window_size', type=int, default=32)
    parser.add_argument('--overlap_ratio', type=float, default=0.5) # æ­¥é•¿ä¸º16ï¼ˆæœ€ä¼˜é…ç½®ï¼‰
    parser.add_argument('--temperature', type=float, default=25.0)
    parser.add_argument('--model_name', type=str, default='KAN-Transformer', help='Name of the model for logging and display.')
    
    # === [çœŸæ­£SOTA] æ¶æ„å‚æ•° ===
    parser.add_argument('--n_layers', type=int, default=5)      # é€‚ä¸­çš„å±‚æ•°
    parser.add_argument('--num_heads', type=int, default=16)    # è¶³å¤Ÿçš„æ³¨æ„åŠ›å¤´
    parser.add_argument('--hidden_space', type=int, default=128) # å¹³è¡¡çš„éšè—ç©ºé—´
    parser.add_argument('--embed_dim', type=int, default=128)   # ç›¸åº”è°ƒæ•´
    
    # === å¹³è¡¡çš„æ­£åˆ™åŒ–å‚æ•° ===
    parser.add_argument('--dropout', type=float, default=0.20)         # é€‚ä¸­çš„dropout
    parser.add_argument('--weight_decay', type=float, default=0.0006)  # é€‚ä¸­çš„weight_decay
    parser.add_argument('--noise_factor', type=float, default=0.004)   # è½»å¾®å™ªå£°
    parser.add_argument('--electrochemical_weight', type=float, default=0.05) # æ¢å¤ä¸ºæ›´ä¿å®ˆçš„0.05
    parser.add_argument('--resistance_loss_factor', type=float, default=0.001, help='Factor for resistance-based stability loss.') # ä¿æŒä¸å˜
    parser.add_argument('--gate_entropy_weight', type=float, default=0.01, help='Entropy penalty to sharpen workload detector.')
    parser.add_argument('--detector_temp_init', type=float, default=0.7, help='Initial temperature for workload detector softmax.')
    
    # === [çœŸæ­£SOTA] è®­ç»ƒå‚æ•° ===
    parser.add_argument('--grid_size', type=int, default=24)       # é«˜ç²¾åº¦KAN
    parser.add_argument('--num_epochs', type=int, default=300)     # å¢åŠ åˆ°300
    parser.add_argument('--batch_size', type=int, default=32)      
    parser.add_argument('--patience', type=int, default=45)        # å¢åŠ åˆ°45
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--lr', type=float, default=0.0004)        # é™ä½å­¦ä¹ ç‡åˆ°0.0004
    parser.add_argument('--gpus', type=int, default=1, help='Number of GPUs to use')
    parser.add_argument('--num_workers', type=int, default=0, help='Number of data loader workers')
    parser.add_argument('--debug_mode', action='store_true', help='Enable debug mode (e.g., fewer epochs, smaller dataset)')
    parser.add_argument('--ckpt_path', type=str, default=None, help='Path to a pre-trained checkpoint to load for testing')
    parser.add_argument('--gradient_accumulation_steps', type=int, default=2)  # è½»å¾®çš„æ¢¯åº¦ç´¯ç§¯
    parser.add_argument('--train_val_split_ratio', type=float, default=0.93, help='Ratio for training set split from total training data.')
    
    args = parser.parse_args()
    args.output_features = [item.strip() for item in args.output_features.split(',')]
    os.makedirs(args.result_dir, exist_ok=True)
    
    print("ğŸ¯ === 25Â°C SOTAè®­ç»ƒ - LA92æµ‹è¯•é…ç½® ===")
    print("ğŸ”„ æ•°æ®é…ç½®è°ƒæ¢:")
    print("   - è®­ç»ƒé›†: 6ä¸ªåŸè®­ç»ƒé›† + NNæµ‹è¯•é›† (å¢å¼ºè®­ç»ƒæ•°æ®)")
    print("   - æµ‹è¯•é›†: LA92 (åŸè®­ç»ƒé›†) + UDDS (ä¿æŒå¯¹æ¯”)")
    print("   - ç›®çš„: éªŒè¯æ¨¡å‹åœ¨LA92å·¥å†µä¸‹çš„æ³›åŒ–èƒ½åŠ›")
    print("ğŸš€ SOTAç­–ç•¥:")
    print("   - ä¿æŒUDDSä¼˜åŠ¿: ç»´æŒç®€å•æœ‰æ•ˆçš„å¤„ç†åˆ†æ”¯")
    print("   - å·¥å†µæ£€æµ‹å™¨: è‡ªåŠ¨è¯†åˆ«ä¸åŒå·¥å†µç±»å‹")
    print("   - è‡ªé€‚åº”æŸå¤±: å·¥å†µç‰¹å®šçš„æŸå¤±æƒé‡")
    print("   - å·¥å†µç‰¹å®šé¢„æµ‹å¤´: ä¸åŒå·¥å†µåˆ†åˆ«ä¼˜åŒ–")
    print("   - ç›®æ ‡: UDDSä¿æŒâ‰¤0.013, LA92<0.025, æ•´ä½“<0.020")
    print(f"   - åŸºå‡†å¯¹æ¯”: åŸç‰ˆå¹³å‡0.021, TabPFNå¹³å‡0.0195")
    
    try:
        pl.seed_everything(args.seed, workers=True)
        datamodule = Electrochemical46FeaturesDataModule(args) # ä½¿ç”¨StandardScalerçš„æ•°æ®æ¨¡å—
        datamodule.setup(stage='fit') # æ˜¾å¼è°ƒç”¨setupï¼Œå¹¶ä¼ å…¥stage
        
        # è·å–å…¨å±€æ ‡å‡†åŒ–å™¨
        scalers_dict = datamodule.scaler
        feature_scaler = scalers_dict['features']

        model = TrueSOTAElectrochemicalLightningModule(args, feature_scaler=feature_scaler) # ä¼ é€’ feature_scaler
        
        # å›è°ƒå‡½æ•°è®¾ç½®
        checkpoint_callback = ModelCheckpoint(
            dirpath=args.result_dir, 
            filename='true-sota-25degC-{epoch:02d}-{val_loss:.6f}',
            save_top_k=1, verbose=True, monitor='val_loss', mode='min'
        )
        early_stop_callback = EarlyStopping(
            monitor='val_loss', patience=args.patience, verbose=True, mode='min'
        )
        lr_monitor = LearningRateMonitor(logging_interval='epoch')
        
        # æ˜¾å¼é…ç½®loggerï¼Œä½¿å…¶ä¿å­˜åˆ°args.result_dir
        from pytorch_lightning.loggers import TensorBoardLogger
        logger = TensorBoardLogger(save_dir=args.result_dir, name='', version='') # nameä¸ºç©ºï¼Œversionä¸ºç©ºï¼Œç›´æ¥ä¿å­˜åˆ°save_dir

        # è®­ç»ƒå™¨è®¾ç½®
        trainer = pl.Trainer(
            max_epochs=args.num_epochs, 
            accelerator='gpu', # æ˜ç¡®æŒ‡å®šä½¿ç”¨GPU
            devices=args.gpus, # ä½¿ç”¨ArgumentParserä¸­å®šä¹‰çš„GPUæ•°é‡
            callbacks=[
                checkpoint_callback, 
                early_stop_callback, 
                lr_monitor,
                MemoryCleanupCallback()
            ],
            precision='32',
            gradient_clip_val=0.5,
            accumulate_grad_batches=args.gradient_accumulation_steps,
            deterministic=False,
            benchmark=True,
            logger=logger # ä¼ å…¥é…ç½®å¥½çš„logger
        )
        
        print(f"\nğŸš€ å¼€å§‹ {args.model_name} è®­ç»ƒ...")

        if args.ckpt_path:
            print(f"   è·³è¿‡è®­ç»ƒï¼Œä»æ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹: {args.ckpt_path}")
            # ç›´æ¥åŠ è½½æ¨¡å‹å¹¶æµ‹è¯•
            model = type(model).load_from_checkpoint(args.ckpt_path, hparams=args) # é‡æ–°ä¼ å…¥hparams
            trainer.test(model, datamodule=datamodule)
        else:
            # æ­£å¸¸è®­ç»ƒæµç¨‹
            trainer.fit(model, datamodule)
            print(f"\nğŸ“Š æµ‹è¯• {args.model_name} æ¨¡å‹...")
            # ä½¿ç”¨æœ€ä½³æ£€æŸ¥ç‚¹è¿›è¡Œæµ‹è¯•
            trainer.test(model, datamodule=datamodule, ckpt_path='best')
        
        print(f"\nâœ… çœŸæ­£SOTAè®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {args.result_dir}")
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()
